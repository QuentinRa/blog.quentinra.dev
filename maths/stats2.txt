Université du Havre - ISEL
Initiation à R - Eric Preud’homme

exercice
---------------------

Idée : donner un affichage (vecteur)
et demander comment on l'as eu.

Strings
--------------

stringi ou stringr

substring
paste
tolower
toupper
chartr // tr en shell
strsplit
grep // grep(pattern = "a" , Texte9, value = FALSE, fixed = TRUE)

cours/théorie
---------------

P : fonction de répartition (Probabilité P(X≤k))
Q : inverse de la fonction de répartition (Quantile)
D : densité pour une loi continue (P(x=k) pour une loi discrète)
R : échantillon aléatoire suivant la loi en question (Random)

FONCTIONS
Lois de proba
P Q D R
Beta  pbeta qbeta dbeta rbeta
Binomiale pbinom qbinom dbinom rbinom
Cauchy pcauchy qcauchy dcauchy rcauchy
Khi-deux pchisq qchisq dchisq rchisq
Exponentielle pexp qexp dexp rexp
F pf qf df rf
Gamma pgamma qgamma dgamma rgamma
Géométrique pgeom qgeom dgeom rgeom
Hypergéométrique phyper qhyper dhyper rhyper
Logistique plogis qlogis dlogis rlogis
Log Normale plnorm qlnorm dlnorm rlnorm
Binomiale négative pnbinom qnbinom dnbinom rnbinom
Normale pnorm qnorm dnorm rnorm
Poisson ppois qpois dpois rpois
Student t pt qt dt rt
Uniforme punif qunif dunif runif
Weibull pweibull qweibull dweibull rweibull

Par exemple pour la loi Normale, la densité est dnorm(x,mu,sigma)
où mu et sigma représentent la moyenne et l’écart-type de la loi.

gamma(n)=(n-1) !

Ainsi pour calculer P(X=2) pour une loi binomiale B(5,1/3) On fait : dbinom(x=2,size=5,prob=1/3)
Pour P(X≤2) on aurait fait : pbinom(q=2,size=5,prob=1/3)
Calculer P(X>2) pour une loi normale N(1,3) : 1-pnorm(q=2,mean=1,sd=3)

tracer
-----------------

persp/contour pour tracer

image(x,y,z) # Pour l’image couleur
contour(x,y,z,add=TRUE) # Pour le tracé des lignes de couleur

library(rgl)
plot3d # interactif

L’option ADD=TRUE permet de superposer au graphique précédent.

boxplot

install.packages(‘GGgally’)
library(GGally)
ggpairs(stid[, c("Taille", "Poids")],aes(colour=stid$Sexe))

utiles
-----------------

head/tail pour voir le début

apply(matrice, 1 ou 2, fonction)
	1 : colonnes, 2 : lignes

maths
---------------------

det : retourne le déterminant d'une matrice
solve : inverse aussi ou
	library(matlib) (A est une 3,3)
	echelon(A, diag(3), verbose=TRUE, fractions=TRUE)
	// Affiche toutes les étapes

5x = 10, que vaut x? solve(5,10)


Rang
	On rappelle que le rang est la dimension de l’image d’une matrice. Si le rang est égal à la taille de
	la matrice, celle-ci est inversible, ses colonnes (et lignes) sont libres, elles forment une base
	de l’espace.

	library(matlib); R(m).

La fonction solve permet de résoudre les systèmes n’ayant pas de « problèmes »,
c’est à dire avec une solution.

La fonction echelon permet de mettre le système sous forme
triangulaire ce qui permet d’achever la résolution des systèmes « à problème » facilement.

showEqn(A,b)
	Voir le système de la matrice A et le vecteur b.

	echelon : mettre sous forme triangulaire. On obtient x1 = ..., x2 = ... etc.

Trace(m)=somme des valeurs propres.

optimisation
-----------------

install.packages("numDeriv")
library(numDeriv)

Montrons que le point (1,1,1) est un point critique de cette fonction.
Pour cela nous calculons le Gradient

grad(func=ma_fonction,x=c(1,1,1))

[1] 0 0 0
Le gradient est nul, le point est donc critique.

Pour déterminer la nature de ce point stationnaire, nous calculons la hessienne :
hessian(func=ma_fonction,x=c(1,1,1))

Il nous reste à déterminer les valeurs propres : eigen(hessian(...))

Les valeurs propres sont positives et négatives, le point (1,1,1) est donc un point selle.

Polynômes
R possède une fonction pour trouver les racines de polynômes (réelles ou complexes) : polyroot
auquel on donne les coeff du polynome (x^0, x^1, ...)

uniroot(f,lower=0,upper=3) avec une fonction continue qui change de signe permet de trouver le 0 (TVI)
	valeur approchée du x pour lequel f(x)~= zéro
  valeur du x pour lequel f(x)~= zéro
  iter : le nombre d’itérations pour arriver au résultat
  estim.prec, l’estimation de la précision de la solution. (peut l'augmenter avec l’option tol=)

integrate
	Calculer des intégrales

	integrate(f, lower = 0, upper = 1)

Il est possible d’approcher les fonctions par des polynômes en utilisant la formule de Taylor.
Le module pracma permet de faire ces calculs.

library(pracma)
taylor(f, 0, 5) // en 0 d'ordre 5

polyval ???

statistiques pondérées
--------------------------

valeurs 1,2,... associés à un pourcentage (ici plutôt un nombre)
-> Désagréger les données pondérées
-> création de deux vecteurs
-> fusion
-> analyse

-> sans désagréger
-> install.packages('questionr')
-> library(questionr)
-> wtd.mean(valeurs, nombre)
-> wtd.var(valeurs, nombre)

structure
-----------------

Les accolades peuvent être omises s’il n’y a qu’une instruction.

Les tableaux commencent à 1

saveRDS (saveRDS(object = ....)) et readRDS pour save des données.

Factor : qualitative
-> quantitative : entiers, ...
-> qualitative : valeur (Homme, Femme, ...)

Factor permet de qualifier les données, par exemple de créer des groupes
disant que 1 corresponds à une valeur et 2 à une autre.
	-> levels()
	-> droplevels()

Si vous avez de nombreuses variables qualitatives dans votre fichier,
il est possible que R les voie comme du texte. Pour éviter de faire factor()
sur chacune d’entre elle : read.csv2("wisc_cancer.csv", stringsAsFactors = TRUE)
Ici nous demandons à R de convertir toutes les variables chaînes de caractères en variables qualitatives.

write.csv2

Sous-partie d'un dataframe
	subset(stid,Sexe==2,select=c(Groupe,Sexe,Note.Fr.))
	-> select : variables gardées
	-> stid : fichier
	-> sexe==2 : condition

library(lattice)
	regroup<-make.groups(nom=valeur, ...)

merge(x, y, by="name")# fusionne les data frames x et y par la colonne name.

order(vecteur) : donne les indexes dans l'ordre

range(variable) : min et max

col=rainbow(10)

esprit d'analyse
----------------------

Superpose histogramme + courbe pour essayer de trouver la loi.

summary() et/ou head().
	-> summary() permet de calculer des statistiques élémentaires sur les variables quantitatives

On peut check les dimensions (dim()) voir combien de données on manipule.
On peut check names() : les noms des colonnes.
str() : voir les types des colonnes et avoir une idée des valeurs.

Pour les autres fonctions, R connaît la variance (var),
l’écart-type (sd) les quantiles (quantile).

On va généralement partitionner les résultats entre
-> apprentissage : analyse/construire un modèle
-> validation : vérifier notre modèle
On veut donc être assuré qu’un seul et même individu n’a pas pu servir à construire ET
à valider le modèle. On parle de validation croisée ou de modélisation supervisée.
-> On peut utiliser sample

En statistique, on est très souvent amené à changer le type des variables. Par exemple,
une variable âge (quantitative) peut être transformée en une variable qualitative
(« Enfant », « Adulte », « Personne âgée ») pour les besoins d’une étude, ou pour
faciliter une interprétation.
-> fait des sélection d'indices et on réécrit les valeurs de la sélection
-> discrétisation non supervisée (on ne sait pas comment découper, laisse R faire)
	-> library(arules)
		-> discretize(v, method = "frequency", breaks = 5)
			-> on veut 5 groupes
		-> discretize(v, method = "interval", breaks = 4)
			-> on veut 4 classes de même amplitude (donc 1-10 11-20 ...
		-> method = "cluster" (homogène)
			-> fait des groupes assez écartés
	-> library(ggplot2)
	-> ggplot(essai, aes(x = Age,fill=Recodx)) + geom_dotplot(dotsize = 3, binwidth=0.9,stackdir = "center")
-> Discrétisation supervisée
	-> on découpe mais en se basant sur une var qualitative (cible)
	-> on va essayer de former des groupes avec un maximum de valeurs homogènes/identiques/proches
	-> l'’algorithme va maximiser le khi-deux de liaison entre les deux variables qualitatives obtenues
	-> library(discretization); res<-chiM(data=essai,alpha=0.05)
-> utilité faire des groupes puis étudier les sous-groupes
-> genre dans un groupe la répartition etc.

Recodage d’une variable qualitative en qualitative
->library(forcats)
-><-fct_recode(stid$Sexe,"H"="1","F"="2") // transforme "1" en "H"
// ou alors on utilise les indexes

-> regouper (utile lorsque Khi deux et que les effectifs théoriques sont inférieurs à 5)
-> library(forcats)
-> fct_collapse(v, "col" = c("old1","old2"))
-> elle sort aussi à regouper des valeurs pareils genre eu, Europ, Europe, ...

La gestion des valeurs manquantes est un très très gros sujet en statistique.
La morale est qu’il vaut mieux ne pas en avoir ! Notre monde étant imparfait,
il va falloir apprendre à les repérer et les gérer.
-> complete.cases(stid)
-> retourne le nombre de NA
-> complete : FALSE : au moins une donnée manquante pour l’individu. TRUE : aucune donnée manquante.

Plusieurs solutions sont possibles : supprimer les individus (la pire), considérer la catégorie
« manquant » comme une valeur à part entière (pour les variables qualis),
remplacer les valeurs manquantes par des valeurs les plus adaptées (en tenant compte des
liaisons avec les autres variables)…
Cette dernière solution porte le doux nom d’imputation des données manquantes. R est très
très fort là dedans, car il possède des extensions dédiées très puissantes. Cela dit,
comme je le disais au début, ce ne sont que des solutions de « rattrapage »,
il vaut mieux TOUT faire pour avoir le moins possible de données manquantes.
-> library(visdat) vis_miss(stid) : voir données manquantes graphiquement

----> Statistiques descriptives
--> Variables qualitatives
----> Diagramme à bandes (barplot)
----> Diagramme à bandes ordonnées (sort puis barplot)
      Parfois, on peut souhaiter trier selon les effectifs (Pareto).
--> Une variable qualitative ordinale : trié les facteurs (levels()[c(1,3,2)]
--> Liaison entre deux variables qualitatives : tableaux croisés
	-> La fonction table permet de faire cela pour les effectifs, prop.table pour les fréquences
	#Pourcentage total prop.table(tableau)
	#Pourcentage ligne prop.table(tableau,1)
	#Pourcentage colonne prop.table(tableau,2)

Variables quantitatives
	* Rappelons qu'une variable quantitative est en général une variable évaluée
	par des nombres réels. Elle est quantifiable.

	Il y en a de deux sortes.
		Les variables quantitatives discrètes qui ne prennent que quelques valeurs
		Les variables quantitatives continues qui peuvent en prendre beaucoup

	Une variable n'est pas quantitative parce qu'elle prends des entiers comme valeurs.
	Et inversement, une variable quantitative n'est pas forcément un nombre.

Le package pivottabler permet de faire des tableaux de ce genre, très sophistiqués.
	install.packages('pivottabler') library(pivottabler)

	qhpvt(notes, rows=c("Groupe"),columns=c("Bac"),
	c("Moy. Français"="mean(Français)","ecart_typ Fra"="sd(Français)"),
	 formats=list("%.1f"), totals=list("Groupe"="Ensemble"))

	totals='totals=NONE'

	Ultra cool ! Genre on peut avoir les moyennes pour chaque filière (colonne) par rapport
	aux Groupes (G1, G2, G3, G4) et ajouter une ligne total avec
	la moyenne pour tous les groupes selon chaque filière.
	La deuxième liste c'est genre des sous-catégories de la colonne (donc pour chaque colonne
	on calcule ça).
	On peut diviser les lignes aussi.

Lorsque le skewness est proche de 0, la distribution est symétrique.
Kurtosis est un coefficient mesurant l'aplatissement
-> on peut voir par exemple si ya autant partout (k=faible) ou alors ya un pic (k=important)
-> Kurtosis=3 = loi gaussienne

Discrète
-> on utilise la fonction barplot en augmentant l’espace entre les barres (space=).

Histogramme des densités (prob=TRUE)
Dans certains cas, il est indispensable de tracer un histogramme des densités
(=effectif/(eff. total * largeur classe)) au lieu des effectifs. Par exemple,
pour des classes de largeur inégales, ou la superposition d'une courbe de densité de proba.
---> au lieu d'avoir un histogramme d'effectifs

ecdf
	-> fonction de répartition
	-> Fonction de répartition avec superposition d’une loi théorique

On voit ici que la taille des STID193 suit approximativement une loi gaussienne.
Cela peut être confirmé par un test de normalité.

Boxplot
	Ils sont pourtant très utiles, car ils représentent sur un seul graphique : min, max, Q1,Q2,Q3
	et la moyenne (haut : Q3 + 1.5h, Q3, Mediane, Q1, Q1-1.5h)
	On rappelle que les valeurs extrêmes sont inférieures à Q1-1.5(Q3-Q1) ou supérieures à Q3+1.5(Q3-Q1).

Le « Taille ~ Groupe » veut dire, que l’on va distinguer la taille selon les groupes.
Attention : Il faut pour cela que Groupe soit qualitative.

Cartographie simple avec R, Carte choroplèthe
	R possède la capacité de représenter des données géographiques grâce à ggplot.
	library(ggplot2) library(dplyr) require(maps) require(viridis)
	world_map <- map_data("france")
	left_join(donnees, france_map, by = "region")

On peut ajouter des droites pour se repérer davantage sur le graphique.
abline(h=mean(stid$Taille)) abline(v=mean(stid$Poids))

Ajustement linéaire
Coefficient de corrélation (cor) et une droite de régression (lm)
coefficients(modele)
Le coefficient de corrélation linéaire est assez élevé (un test le prouverait).
Le nuage a une forme allongée, on peut donc effectuer un ajustement linéaire.
Il est possible de récupérer les valeurs des résidus (residuals(modele)) et les valeurs
y prédites (fitted(modele)).

Matrices de graphiques (pairs, GGally)
C’est utile pour étudier la liaison entre plusieurs variables numériques

Matrice des corrélations
Ces matrices sont utiles pour repérer les liens entre les variables quantitatives.
Les coefficients de corrélation linéaire sont calculés et représentés sous forme de carrés
de couleurs Pour cela, nous avons besoin d’une extension corrplot.
library(corrplot)
corrplot(M, method="circle")
R possède des graphiques intéressants pour représenter simultanément des variables qualitatives et quantitatives.

Statistique inférentielle (tests)
Le but des statistiques descriptives est de décrire une population connue.
Celui des statistiques inférentielles est de donner des résultats sur une population inconnue
à partir d’un échantillon (connu). C’est plus difficile et nécessite le recours aux probabilités.

Test d’indépendance du Khi deux – Test de Fisher
Ce test est utilisé pour étudier l’indépendance de deux variables qualitatives.
C’est un des tests les plus célèbres
----> recherche si ya une relation entre des variables

Si les effectifs théoriques de chaque cellule sont supérieurs ou égaux à 5 alors T suit
approximativement une loi du ² à (p-1)(q-1) degrés de liberté. Dans le cas contraire,
il faut opérer des regroupements ou utiliser un test exact de Fisher. Certains n’utilisent
d’ailleurs que ce test exact.
> chisq.test(tableau,correct=FALSE)
Si p-value ~= 0.05 alors possiblement faux sinon indépendantes.
Ceci est confirmé par le calcul des résidus
	* Calcul des résidus (contributions)
	* regarde le $expected
	Leurs carrés correspondent aux contributions. Plus la valeur du résidu est élevée, plus la cellule joue
	un rôle dans la liaison entre les variables. Si le résidu est négatif, les modalités se repoussent, si
	le résidu est positif, elles s’attirent.

	Formule de calcul des résidus : (observed - expected) / sqrt(expected).
	test$residuals

Test exact de Fisher
Dans le cas où les effectifs théoriques sont inférieurs à 5, il faut changer de test.
> fisher.test(tableau)
attention test gourmand

Test du Khi deux d’adéquation ou de conformité
L’utilité de ces tests est de comparer une distribution observée à une distribution théorique connue
(normale, uniforme, poisson etc...) . Le but étant de vérifier, en général, les conditions
d'application d'autres tests.
>
Pour cela nous allons mesurer "l'écart" entre les valeurs observées et les valeurs théoriques
que l'on aurait si H0 était vraie. Ensuite nous comparons cet "écart" à des valeurs critiques...
> chisq.test(nos_resultats , p = théorique)

On souhaite voir si cette distribution est proche de la loi de Poisson. On va donc ajuster une distribution
de Poisson à ces données. Nous choisirons une loi de paramètre 3 puisque
la moyenne de notre échantillon donne 3. Le calcul des effectifs théoriques donne.

c2<-sum((Freq-Chtheo)^2/Chtheo)
Pval<-1-pchisq(c2,df=5-1-1) // df = degré de liberté. Je crois ya 5 groupes mais -1 et -1 ?
> (p-1)(q-1) degrés de liberté
> (1,2,...,p) et (1,2,...,q) de modalités
> k-r-1 degrés de liberté où r est le nombre de paramètres de F0 estimés à partir des observations

Tests de normalité
Ce test a pour but de vérifier la normalité d’une population parent à partir d’un échantillon.
Il sert, en général, de préliminaire à un autre test (paramétrique).

R sait faire par défaut le test de Shapiro-Wilk. Si nous en voulons d’autres, il faut activer
l’extension « nortest ». Nous retrouvons alors Anderson Darling, Cramer von Mises etc.

Pour tous ces tests, l’hypothèse H0 est la normalité de la distribution. Si nous rejetons H0,
la distribution parent a de fortes chances de ne plus être gaussienne.

shapiro.test : dessus 5% alors probablement gaussienne
library(nortest) ad.test(arbres$Hauteurs) Anderson-Darling normality test
cvm.test(arbres$Hauteurs) Cramer-von Mises normality test

Un test ne suffit JAMAIS pour conclure. Un graphique du type Droite de Henry est le complément indispensable.

qqnorm(arbres$Hauteurs,xlab="Fractiles",ylab="Hauteurs",col="blue", main="Droite de Henry pour la variable Hauteurs",datax=TRUE) qqline(arbres$Hauteurs,datax=TRUE,col="red")

R trace la fonction de répartition avec une échelle des ordonnées spéciale
« gausso-arithmétique » (Y=aX+b si X est gaussien). Avec cette échelle, la fonction de répartition des distributions
normales est représentée par une droite appelée « droite de Henry ».
Par conséquent plus « l'allure » de la fonction de répartition de votre échantillon est linéaire
plus cette distribution vérifie l'hypothèse de normalité.

Tests sur les proportions (variable qualitative)
	Test de comparaison d’une proportion à une valeur fixée, intervalle de confiance d’une proportion
		Je crois on veut tester si un truc existe dans un échantillon.

		prop.test(x,n,p0,correct=TRUE ou FALSE), alternative="two.sided" (default),
		 "greater" or "less".)

		 correct= sert à demander s’il doit y avoir correction de continuité
		 alternative = sert à choisir l’hypothèse H1 : p=p0 ou p>p0 ou p<p0.

		 Binomiale : H0
		 Binomiale bilatéral : H1 jcrois

		 Ex: 102 succès sur 140 avec proba 0.8
		 prop.test(102,140,p=0.8,correct=FALSE)
		 p-value < 5%  nous rejetons H0. p ne peut être égal à 80% au risque <5%*
		 # binom.test(102,140,p=0.8) # alternative

	Test de comparaison de deux proportions
		Si on a deux lois avec x1 succès sur N1 et x2 sur N2
		prop.test(c(x1,x2),c(N1,N2),correct=FALSE)
		La pvaleur est très nettement inférieure à 5%. Il y a donc une différence très significative entre
		x1 et x2.

	Tests d'égalités de variances (Fisher, Bartlett)
		Préliminaire pour tester le test précédent.

		Les populations sont supposées normales18 et les échantillons qui en sont issus sont aléatoires et prélevés indépendamment19les uns des autres.

		1. Test de Fisher (comparaison de 2 variances)
			var.test(Note.Fr.~Sexe,data=stid)

		2. Test de Bartlett (comparaisons multiples de variances)
		Ce test permet de tester l'égalité de plusieurs variances simultanément
		(Fisher ne permet d'en tester que 2) mais il est aussi moins puissant.
			bartlett.test(vecteur1(quantitatif),vecteur2(qualitatif))

	Tests d’égalité de moyennes (Student)
		1. Test de Student de Comparaison d’une moyenne à une valeur fixée et intervalle de confiance

		Notre échantillon étant de taille supérieure à 30, pas besoin de vérifier la normalité.

		Nous allons ici comparer une moyenne à une valeur fixée à l’avance (indépendamment de l’échantillon).
    Typiquement utilisé en contrôle de qualité

		t.test(isel03$Taille,alternative="two.sided", mu=170)
		Pour obtenir un simple intervalle de confiance de la moyenne il suffit de donner à R : t.test(isel03$Taille)$conf.int

		2. Test de Student de comparaison de 2 moyennes

		#T Test en supposant les variances égales (vérifié auparavant avec Fisher)
		t.test(x=arbres1,y=arbres2,alternative="two.sided",var.equal=TRUE)

		3. Test T pour données appariées (Paired)
		Lorsque les échantillons ne sont pas indépendants, le test précédent n’est plus applicable.

		Un plan B a été inventé dans le cas où les 2 échantillons n’en sont qu’un seul : par exemple
		lorsque des mêmes individus ont été mesurés deux fois. Dans ce cas, nous calculons la différence
		pour chaque individu (cohérent car ce sont les mêmes) et nous testons la nullité.



Stop page 243/451