Définition: Le but principal de la statistique est de déterminer les caractéristiques d'une population
donnée à partir de l'étude d'une partie de cette population, appelée "échantillon" ou "échantillon
représentatif". La détermination de ces caractéristiques doit permettre aux statistiques d'être un outil
d'aide à la décision!

Remarque: Le traitement des données concerne la "statistique descriptive". L'interprétation des
données à partir des estimateurs s'appelle "l'inférence statistique" (ou "statistique inférentielle"), et
l'analyse de données en masse la "statistique fréquentielle" (en opposition à l'inférence bayésienne).

Lorsque nous observons un événement prenant en compte certains facteurs, il peut arriver qu'une
deuxième observation ait lieu dans des conditions qui semblent identiques. En répétant ces mesures
plusieurs fois sur différents objets supposés similaires, nous pouvons constater que les résultats
observables sont distribués statistiquement autour d'une valeur moyenne qui est, finalement le résultat
possible le plus probable. Dans la pratique, nous n'effectuons cependant parfois qu'une seule mesure et
il s'agit alors de déterminer la valeur de l'erreur que nous commettons en adoptant celle-ci comme
moyenne mesurée. Cette détermination nécessite de connaître le type de distribution statistique auquel
nous avons affaire et c'est ce que nous allons nous attarder (entre autres) à étudier ici (les bases du
moins!). Il existe cependant plusieurs approches méthodologiques courantes (les moins courantes
n'étant pas citées pour l'instant) face au hasard

Vincent ISOZ [v3.0 - 2013]

empiriques = grande population ?

3. Enfin, quand nous pouvons lier des probabilités aux événements aléatoires, soit que ces probabilités
découlent de calculs ou de mesures, soit qu'elles reposent sur une expérience acquise auprès de
situations antérieures de même nature que la situation actuelle, nous pouvons faire appel aux
statistiques descriptives et inférentielles (contenu du présent chapitre) pour tirer des informations
exploitables et pertinentes de cette masse de données acquises.

R1. Sans la statistique mathématique, un calcul sur des données (par exemple une moyenne), n'est
qu'un "indicateur ponctuel". C'est la statistique mathématique qui lui donne le statut d'estimateur
dont on maîtrise le biais, l'incertitude et autres caractéristiques statistiques. Nous cherchons en
général à ce que l'estimateur soit sans biais, convergeant et efficace (nous verrons lors de notre
étude des estimateurs plus loin de quoi il s'agit exactement).
R2. Lorsque nous communiquons une statistique il devrait être obligatoire de préciser l'intervalle de
confiance ainsi que la taille de l'échantillon étudié et ses caractéristiques détaillées sinon quoi elle
n'a quasiment aucune valeur scientifique.

---------------->
Il faut que l'échantillon soit représentatif de la population
Pour cela, l'échantillonnage aléatoire est le meilleur moyen d'y parvenir.

D1. Nous sommes en présence d'un "caractère quantitatif" lorsque chaque élément observé fait
explicitement l'objet d'une même mesure. A un caractère quantitatif donné, nous associons une
"variable quantitative" continue ou discrète qui synthétise toutes les valeurs possibles que la mesure
considérée est susceptible de prendre (ce type d'information étant représenté par des courbes de loi de
Gauss-Laplace, de la loi bêta, de la loi de Poisson, etc.).

D2. Nous sommes en présence d'un "caractère qualitatif" lorsque chaque élément observé fait
explicitement l'objet d'un rattachement unique à une "modalité" choisie dans un ensemble de modalités
exclusives (de type: homme | femme) permettant de classer tous les éléments de l'ensemble étudié selon
un certain point de vue (ce type d'information étant représenté par des diagrammes à barre, fromages,
diagrammes à bulles, etc.). L'ensemble des modalités d'un caractère peut être établi a priori avant
l'enquête (une liste, une nomenclature, un code) ou après enquête. Une population étudiée peut être
représentée par un caractère mixte, ou ensemble de modalités tel que genre, tranche salariale, tranche
d'âge, nombre d'enfants, situation matrimoniale par exemple pour un individu.

D3. Un "échantillon aléatoire" est un échantillon tiré au hasard dans lequel tous les individus d'une
population ont la même chance, ou "équiprobabilité" (et nous insistons sur le fait que cette probabilité
doit être égale), de se retrouver dans l'échantillon.

D4. Dans le cas contraire d'un échantillon dont les éléments n'ont pas été pris au hasard, nous disons
alors que l'échantillon est "biaisé" (dans le cas inverse nous disons qu'il est "non-biaisé")

La notion de "moyenne" ou "tendance centrale" est avec la notion de "variable" à la base des statistiques.

D1. La "moyenne arithmétique" ou "moyenne empirique" (xi/n)

moyenne pondérée par les effectifs (xi * nombre d'occurrences)/n
= d'espérance mathématique (en proba)

fréquence des classes ni/n

moyenne pondérée par les fréquences de classe fi*xi

un intervalle de classe est fermé à gauche et ouvert à droite: [...,...[.
règle de Sturges"
il est souvent utile et nécessaire de
regrouper les mesures/données dans des intervalles de classe de largeur donnée

Le "mode", noté Mod ou simplement M, est par définition la valeur qui apparaît le plus
grand nombre de fois dans une série de valeurs.

D2. La "médiane" ou "moyenne milieu", notée (ou plus simplement M), est la valeur qui coupe une
population en deux parties égales.

dispersion des écarts absolus : somme des écarts absolus

Il existe un autre cas pratique où le statisticien n'a à sa disposition que des valeurs regroupées sous
forme d'intervalles de classes statistiques. La procédure pour déterminer la médiane est alors différente:
Lorsque nous avons à notre disposition uniquement une variable classée, l'abscisse du point de la
médiane se situe en général à l'intérieur d'une classe. Pour obtenir alors une valeur plus précise de la
médiane, nous procédons à une interpolation linéaire. C'est ce que nous appelons la "méthode
d'interpolation linéaire de la médiane".

La valeur de la médiane peut être lue sur le graphique (F(x)) ou calculée analytiquement.
La valeur de la médiane M se trouve évidemment au croisement entre la probabilité de 50% (0.5) et
l'abscisse.
M = a + (b-a) * \frac{0.5-F(a)}{F(b)-F(a)}

Pour éviter d'obtenir une moyenne arithmétique ayant peu de sens, nous calculons
souvent une "moyenne élaguée", c'est à dire une moyenne arithmétique calculée après avoir enlevé
des valeurs aberrantes à la série.

Les "quantiles" généralisent la notion de médiane en coupant la distribution en des ensembles donnés
de parties égales (de même cardinal pourrions-nous dire...) ou autrement dit en intervalles réguliers.
Nous définissons ainsi les "quartiles", les "déciles" et les "centiles" (ou "percentiles") sur la population,
ordonnée dans l'ordre croissant, que nous divisons en 4, 10 ou 100 parties de même effectif.

Nous parlerons ainsi du centile 90 pour indiquer la valeur séparant les premiers 90% de la population
des 10% restants.

Ce concept est très important dans le cadre des intervalles de confiance que nous verrons beaucoup
plus loin dans ce chapitre et très utile dans le domaine de la qualité avec l'utilisation des boîtes à
moustaches (traduction de Box & Whiskers Plot ou BoxPlot) permettant de comparer ("discriminer"
comme disent les spécialistes) rapidement deux populations de données ou plus et surtout d'éliminer les
valeurs aberrantes (prendre comme référence la médiane sera justement plus judicieux!):

Une autre représentation mentale très importante des boîtes à moustache est la suivante (elle permet
donc de se donner une idée de l'asymétrie de la distribution):

D3. Par analogie avec la médiane, nous définissons la "médiale" comme étant la valeur (dans l'ordre
croissant des valeurs) qui partage la somme (cumuls) des valeurs en deux masses égales (donc la
somme totale divisée par deux).

Dans le cas de salaires, alors que le médiane donne le 50% des salaires se trouvant en-dessous et
en-dessus, la médiale donne combien de salariés se partagent (et donc le salaire partageant) la première
moitié et combien de salariés se partagent la seconde moitié de l'ensemble des coûts salariaux.

La "moyenne quadratique" parfois simplement notée Q qui est définie par: ...
racine m-ième de la somme des xi^m/n. L'écart type est une moyenne quadratique.

moyenne pondérée
en statistiques pour calculer une espérance (le dénominateur étant
toujours égal à l'unité en probabilités)

2.1. LISSAGE DE LAPLACE (309)

Pour en revenir à nos fréquences de classes vues bien plus haut et avant de continuer avec l'étude de
quelques propriétés mathématiques des moyennes... il faut savoir que lorsque nous travaillons avec des
lois discrètes de probabilités il arrive très (très) fréquemment que nous rencontrions un problème
typique dont la source est la taille de la population. Considérons comme exemple le cas où nous avons
12 documents et que souhaiterions estimer la probabilité d'occurrence du mot "Viagra". Nous avons sur
un échantillon les valeurs suivantes

Et ici nous avons un phénomène courant. Il n'y a aucun document avec 5 occurrences du mot qui nous
intéresse. L'idée (très courante dans le domaine du Data Mining) est alors d'ajouter artificiellement et
empiriquement un comptage en utilisant une technique appelée "lissage de Laplace" qui consiste à
additionner k unités à chaque occurrence et qui est courant dans le domaine du Data Mining. Dès lors le
tableau devient: (ex: ajouter 1 à tout le monde)

P2. La moyenne arithmétique a une propriété de linéarité
P3. La somme pondérée des écarts à la moyenne arithmétique est nulle.

existait trois types de variables quantitatives très importantes qu'il convient absolument de différencier:
1. Les variables discrètes (par comptage): Sont analysées avec des lois statistiques basées sur un
domaine de définition dénombrable toujours strictement positif (loi de Poisson typiquement dans
l'industrie). Sont quasiment toujours représentées sous forme graphique par des histogrammes.

2. Les variables continues (par mesure): Sont analysées avec des lois statistiques basées sur un domaine
de définition non dénombrable strictement positif ou pouvant prendre toute valeur positive ou négative
(loi Normale typiquement dans l'industrie). Sont également quasiment toujours représentées sous forme
graphique par des histogrammes avec des intervalles de classe.

3. Les variables par attribut (de classification): Il ne s'agit pas de données numériques mais de données
qualitatives de type {Oui, Non}, {Réussi, Échec}, {A temps, En retard}, etc. Les données de type
attribut suivent une loi Binomiale

Soit X une variable aléatoire (v.a.) numérique (quantitative). Elle est complètement décrite par la
valeur de la probabilité (pour les variables discrètes) ou par la probabilité cumulée (pour les variables
continues)
Cette probabilité (cumulée) F(x) s'appelle la "fonction de répartition" de la variable X.
P(a <= x <= b) = F(b) - F(a)

fonction de répartition empirique

Définition: Nous définissons "l'espérance mathématique", appelée aussi "moment d'ordre 1",
appelée aussi "règle des parties".
En d'autres termes, nous savons qu'à chaque événement de l'espace des échantillons est associé une
probabilité à laquelle nous associons également une valeur (donnée par la variable aléatoire). La
question étant alors de savoir quelle valeur, à long terme, nous pouvons obtenir. La valeur espérée,
(l'espérance mathématique donc...) est alors la moyenne pondérée, par la probabilité, de toutes les
valeurs des événements de l'espace des échantillons.

E(aX) = aE(X)
E(X+Y) = E(X)+E(Y)
E(c) = c

Définition: Après avoir traduit la tendance par l'espérance il est intéressant de traduire la dispersion ou
"déviation standard" autour de l'espérance par une valeur appelée "variance de X" ou encore "moment
centré du deuxième ordre", notée V(X) ou sigma^2_X (lire "sigma-deux")

La variance n'est cependant pas comparable directement à la moyenne, car l'unité de la variance est le
carré de l'unité de la variable, ce qui découle directement de sa définition. Pour que l'indicateur de
dispersion puisse être comparé aux paramètres de tendance centrale (moyenne, médiane et... mode), il
suffit d'en prendre la racine carrée.
L'écart-type est donc la moyenne quadratique des écarts (ou "écart moyen quadratique") entre les
observations et leur moyenne.

coefficient de variation : sigma/u

"L'écart absolu moyen" (moyenne des valeurs absolues des écarts à la moyenne):
qui est un indicateur élémentaire très utilisé lorsque nous ne souhaitons pas faire de l'inférence
statistique sur une série de mesures

"La déviation absolue de la médiane" noté MAD (médiane des valeurs absolues des écarts à la
médiane):
qui est considéré comme un indicateur plus robuste de la dispersion que ceux donnés par l'écart absolu
moyen ou l'écart-type (malheureusement cet indicateur n'est pas intégré nativement dans les tableurs).
----> on retire la médiane a toute les valeurs.
---> MAD = médiane après sort()

Le terme de la somme se trouvant dans l'expression de la variance (écart-type) est appelée "somme des
carrés des écarts à la moyenne". Nous l'appelons aussi la "somme des carrés totale", ou encore la
"variation totale" dans le cadre de l'étude de l'ANOVA (voir la fin de ce chapitre).

variable centrée réduite
Y = (X - mu)/sigma

Ainsi, toute répartition statistique définie par une moyenne et un écart-type peut être transformée en
une autre distribution statistique souvent plus simple à analyser.

V(nX) = n^2V(x)
V(X+Y) = V(X) + V(Y) + 2cov(X,Y)

Nous venons de voir dans la dernière relation le concept de "covariance" dont nous verrons une
expression plus commode un peu plus bas mais donc définie par:

Remarque: Les statistiques peuvent être découpées selon le nombre de variables aléatoires que
nous étudions. Ainsi, lorsqu'une seule variable aléatoire est étudiée, nous parlons de "statistique
univariée", pour deux variables aléatoires de "statistique bivariée" et en général, de "statistique
multivariée".

En effet si, en général X et Y
croissent simultanément, les produits seront positifs (corrélés positivement), tandis
que si Y décroît lorsque X croît, ces même produits seront négatifs (corrélés négativement).

Maintenance, considérons un vecteur de composantes et un autre vecteur de
composantes , tous deux étant des variables aléatoires, le calcul de la covariance des
composantes deux à deux donne ce que l'on appelle la "matrice des covariances"

Remarque: Donc plus la covariance est faible, plus les séries sont indépendantes. A l'inverse, plus la
covariance est élevée, plus les séries sont liées.
si X, Y sont indépendantes cov=0

3.1.3. ESPÉRANCE ET VARIANCE DE LA MOYENNE (ERREUR STANDARD ET FCP)
Souvent en statistique, il est utile de déterminer l'écart-type de la moyenne empirique (ou en d'autres
termes...: l'erreur quadratique moyenne). Voyons de quoi il s'agit:

en gros si sont identiquement distribuées et indépendantes
alors E(X) = 1/n * n mu
sinon  1/n * somme des espérances

variance : 1/n^2
V(x) = sigma^2/n
--> d'où l'écart-type de la moyenne appelé aussi "erreur-type", "erreur-standard" ou encore "variation non systématique":
sigma(x) = sigma/sqrt(n)

la loi de probabilité de la
variable aléatoire X, moyenne de n variables aléatoires identiquement distribuées et linéairement
indépendantes, est alors la loi:
N(mu, sigma/sqrt(n))

nous allons démontrer cette relation immédiatement car l'utilisation de la covariance seule pour
l'analyse des données n'est pas géniale car elle n'est pas à proprement parler bornée et simple d'usage
(au niveau de l'interprétation). Nous allons donc construire un indicateur plus facile d'usage en
entreprise.

Finalement nous obtenons une forme de l'inégalité statistique dite "inégalité de Cauchy-Schwarz"
-1 < ... < 1

Si les variances de X et Y sont non nulles, la corrélation entre X et Y est définie par le "coefficient de
corrélation linéaire" (il s'agit donc de la covariance standardisée afin que son amplitude ne soit pas
dépendante de l'unité de mesure choisie):

Il traduit la plus ou moins grande dépendance linéaire
de X et Y et ou, géométriquement, le plus ou moins grand aplatissement. Nous pouvons donc dire qu'un
coefficient de corrélation nul ou proche de 0 signifie qu'il n'y a pas de relation linéaire entre les
caractères. Mais il n'entraîne aucune notion d'indépendance plus générale.

Quand le coefficient de corrélation est proche de 1 ou -1, les caractères sont dits fortement corrélés. Il
faut prendre garde à la confusion fréquente entre corrélation et causalité. Cependant, que deux
phénomènes soient corrélés n'implique en aucune façon que l'un soit cause de l'autre.

----> -1
nous avons affaire à une corrélation négative dite "corrélation négative parfaite" (dans la
cas d'une relation linéaire tous les points de mesures sont situés sur une droite de pente négative).
----> 1
nous avons affaire à une corrélation négative ou positive dite "corrélation imparfaite"
(dans la cas d'une relation linéaire tous les points de mesures sont situés sur une droite de pente

L'analyse du coefficient de corrélation poursuit donc l'objectif de déterminer le degré d'association
entre les différentes variables: celui-ci est souvent exprimé par le coefficient de détermination, qui est
le carré du coefficient de corrélation. Le coefficient de détermination mesure donc la contribution d'une
des variables à l'explication de la seconde.

D1. Nous disons que X est une variable continue si sa "fonction de répartition" est continue (déjà
définie plus haut).
la "fonction de survie" (survival function) ou "fonction de queue" : 1-F(x)

Il est intéressant de remarquer que la définition amène à ce que la probabilité qu'une
variable aléatoire totalement continue prenne une valeur donnée est nulle! Donc ce n'est pas parce
qu'un événement a une probabilité nulle qu'il ne peut arriver!!!

----------- 338

3.3. POSTULAT FONDAMENTAL DE LA STATISTIQUE
Le but ultime de la statistique est de remonter de l'échantillon à la fonction de répartition analytique qui
lui aurait donné naissance.

Diversité (ex: vendeurs/produits et le taux de divsersité)
Formule de Shannon
N = nombre de produits
N log(N) - (freq_prod_i * log(freq_prod_i))
/
N

5. FONCTIONS DE DISTRIBUTIONS = loi
Lorsque nous observons des phénomènes probabilistes, et que nous prenons note des valeurs prises par
ces derniers et que nous les reportons graphiquement, nous observons toujours que les différentes
mesures obtenues suivent une caractéristique courbe ou droite typique fréquemment reproductible.
Dans le domaine des probabilités et statistiques, nous appelons ces caractéristiques des "fonctions de
distribution" car elles indiquent la fréquence avec laquelle la variable aléatoire apparaît avec certaines
valeurs.

Ces fonctions sont en pratique bornées par ce que nous appelons "l'étendue de la distribution", ou
"dispersion de la distribution", qui correspond à la différence entre la donnée maximale (à droite) et la
donnée minimale (à gauche) des valeurs observées (max-min).

----------- 342

D1. La relation mathématique qui donne la probabilité qu'a une variable aléatoire d'avoir une valeur
donnée de la fonction de distribution est appelée "fonction de densité", "fonction de masse" ou encore
"fonction marginale".

D2. La relation mathématique qui donne la probabilité cumulée qu'a une variable aléatoire d'être
inférieure ou égale à une certaine valeur est nommée la "fonction de répartition" ou "fonction
cumulée".

D3. Des variables aléatoires sont dites "indépendantes et identiquement distribuées" (i.i.d.) si elles
suivent toutes la même fonction de distribution et qu'elles sont indépendantes...

---> P(A) = 1/N : "fonction discrète uniforme" (ou "loi discrète uniforme")
E(X) = N+1/2
E(X) = N^2-1/12

----> FONCTION DE BERNOULLI
Ainsi, une variable aléatoire X suit une "fonction de Bernoulli" (ou "loi de Bernoulli") si elle ne peut
prendre que les valeurs 0 ou 1, associées aux probabilités p et q

Remarquons que par extension, si nous considérons N événements où nous obtenons dans un ordre
particulier k fois une des issues possible (réussite) et N-k l'autre (échec), alors la probabilité d'obtenir
une telle série (de k réussites et N-k échecs ordonnés dans un ordre particulier) sera donnée par:
P(x=k) : p^k q^{N-k}
E(x) = p
V(x) = p * q

----> FONCTION GÉOMÉTRIQUE
La loi géométrique ou "loi de Pascal" consiste dans une épreuve de type Bernoulli, dont la probabilité
de succès est p et celle d'échec q=1-p sont constantes, que nous renouvelons de manière
indépendante jusqu'au premier succès.
P(x=k) q^{k-1}*p

E(x)= 1/p
V(x)= (1-p)/p^2

----> FONCTION BINOMIALE

Si nous revenons maintenant à notre épreuve de Bernoulli. Plus généralement, tout N-uplet particulier
formé de k succès et de N-k échecs aura pour probabilité (dans le cadre d'un tirage avec remise ou sans
remise si la population est grande en première approximation...):
P(x=k) = p^k q^{N-k} * (C^k_k)
d'être tiré (ou d'apparaître) quel que soit l'ordre d'apparition des échecs et réussites.

E(x) = np
V(x) = p(1-p)/n

----> FONCTION BINOMIALE NÉGATIVE

La loi binomiale négative s'applique dans la même situation que la loi binomiale mais elle donne la
probabilité d'avoir E échecs avant la R-ème réussite quand la probabilité de succès est p (ou
inversement la probabilité d'avoir R réussites avant le E-ème échec quand la probabilité d'échec est p).

P(ok) = p P(ko)= 1-p = q
p^{nok} * q^{nko} * C^N_k

ce qui correspond donc à la probabilité d'avoir nko échecs avant la nok-ème réussite.

E(x) = 1/p
V(x) = q/p^2

----> FONCTION HYPERGÉOMÉTRIQUE

V(x) = kpq * fpc^2
V(x) = sqrt(kpq) * fpc

fpc = "facteur de correction de population" ou
      en anglais "finite population correction factor"
      = (n-k)/(n-1)

----> FONCTION DE POISSON

Pour certains événements forts rares, la probabilité p est très faible et tend vers zéro. Toutefois la valeur
moyenne n*p tend vers une valeur fixe lorsque n tend vers l'infini.

Nous obtenons ainsi la "fonction de Poisson" (ou "loi de Poisson"), appelée également parfois "loi des
événements rares", donnée donc par:
E(x) = np

Le fait important que pour la loi de Poisson nous ayons la variance qui soit égale à l'espérance est
appelé "propriété d'équidispersion de la de Poisson". Il s'agit d'une propriété souvent utilisée dans la
pratique comme indicateur pour identifier si des données (à support discret) sont distribuées selon une
loi de Poisson.

----> FONCTION DE GAUSS-LAPLACE/LOI NORMALE

Cette caractéristique est la plus importante fonction de distribution en statistiques suite au résultat d'un
théorème connu appelé "théorème central limite" qui comme nous le verrons, permet de démontrer
(entre autres) que toute suite de variables aléatoires indépendantes de même loi ayant une espérance et
un écart-type fini et non nécessairement égales converge vers une fonction de Gauss-Laplace (loi
Normale).

Pour information, une variable suivant une loi Normale centrée réduite est très souvent par tradition
notée Z (pour "Zentriert" en allemand).
En revenant aux variables non normées:

nous obtenons donc la "fonction Gauss-Laplace" (ou "loi de Gauss-Laplace") ou également appelée
"loi Normale" donnée sous forme de densité de probabilité par:
k-u/sigma

Interval 1 sigma (-sigma, sigma) : ~68.3%
Interval 2 sigma (-2sigma, 2sigma) : ~95.4%
Interval 3 sigma (-3sigma, 3sigma) : ~99.7%

La largeur de l'intervalle a une très grande importance dans l'interprétation des incertitudes d'une
mesure. La présentation d'un résultat comme N+n*sigma=p% signifie que la valeur moyenne a environ p% de
chance (probabilité) de se trouver entre les limites N-n*sigma,N+n*sigma

La loi de Gauss-Laplace n'est par ailleurs pas qu'un outil d'analyse de données mais également de
génération de données.

Le fait que la somme de deux lois Normales donne toujours une loi Normale est ce que nous nommons
en statistiques la "stabilité par la somme" de la loi de Gauss-Laplace.

-----> LOI NORMALE CENTRÉE RÉDUITE

k* = k-mu/sigma
LNCR(mu=0,sigma=1)

henry coeff de la droite
0.01k-2
sachant égal à k-mu/sigma
donc mu=200
sigma=100

-----> DIAGRAMME QUANTILE-QUANTILE

Une autre manière de juger qualitativement de l'ajustement de données expérimentales avec une loi
théorique (quelle qu'elle soit!!!) est l'utilisation d'un "diagramme quantile-quantile".

L'idée est assez simple, il s'agit de comparer les données expérimentales, aux données théoriques
supposées suivre une loi donnée.

Et bien évidemment on peut comparer les quantiles observés à toute loi théorique supposée. Plus les
points seront alignés sur la droite, meilleur sera l'ajustement! C'est très visuel, très simple et beaucoup
utilisé par les non spécialistes en statistiques dans les entreprises.

---> FONCTION LOG-NORMALE
y = ln(x)

nous voyons que y suit une fonction de probabilité de type loi Normale de moyenne mu et de variance
sigma^2 (moments de la loi Normale).

---> FONCTION UNIFORME CONTINUE

Soient a < b . Nous définissons la fonction de distribution de la "fonction uniforme" (ou "loi uniforme")
par la relation

P(x) = 1/(b-a) indicatrice[a,b]

E(x) : (a+b)/2
V(x) : (b-a)^2/12

Cette fonction est souvent utilisée en simulation dans les entreprises pour signaler que la
variable aléatoire a des probabilités égales d'avoir une valeur comprise dans un certain intervalle.

---> FONCTION DE PARETO

La "fonction de Pareto" (ou "loi de Pareto"), appelée aussi "loi de puissance" ou encore "loi scalante"
est la formalisation du principe des 80-20. Cet outil d'aide à la décision détermine les facteurs (environ
20%) cruciaux qui influencent la plus grande partie (80%) de l'objectif.

1 - (x_m/x)^k avec x >= x_m

---> FONCTION EXPONENTIELLE

Nous définissons la "fonction exponentielle" (ou "loi exponentielle") par la relation de fonction de
distribution suivante:
lambda * e^{-lambdax} * indicatrice[0,+inf]

E(x) = 1/lambda
V(x) = 1/lambda^2

---> FONCTION DE CAUCHY

Soient X,Y deux variables aléatoires indépendantes suivant des lois Normales centrées réduites
(variance unité et espérance nulle).

Pas d'espérance.

---> LOI BÊTA
---> LOI FONCTION GAMMA

Beta(a)/lambda^a

---> FONCTION DE KHI-DEUX (OU DE PEARSON)

La "fonction de Khi-deux" (appelée aussi "loi du Khi-deux" ou encore "loi de Pearson") n'est par
définition qu'un cas particulier de la fonction de distribution Gamma dans le cas a = k/2 où et
lambda = 1/2, avec k entier positif

E(k) = 2k

pour indiquer que la distribution de la variable aléatoire X est la loi du Khi-deux. Par ailleurs il est
courant de nommer le paramètre k "degré de liberté" et de l'abréger "ddl".

---> FONCTION DE STUDENT T

La "fonction de Student" (ou "loi de Student") de paramètre k est définie par la relation
avec k étant le degré de liberté de la loi du Khi-deux sous-jacente à la construction de la fonction de
Student comme nous allons le voir.

---> FONCTION DE FISHER

. Les paramètres k et l sont des entiers positifs et correspondent aux degrés de liberté des deux
lois du Khi-deux sous-jacentes. Cette distribution est souvent notée F(k,l) et peut être obtenue

==========================================
ESTIMATEURS DE VRAISEMBLANCE
==========================================

Nous supposons que nous disposons d'observations xi qui sont des réalisations de variables
aléatoires non biaisées (dans le sens qu'elles sont choisies aléatoirement parmi un lot) indépendantes
Xi de loi de probabilité inconnue mais identique.

Nous allons chercher à estimer cette loi de probabilité P inconnue à partir des observations

Supposons que nous procédons par tâtonnements pour estimer la loi de probabilité P inconnue. Une
manière de procéder est de se demander si les observations avaient une probabilité
élevée ou non de sortir avec cette loi de probabilité arbitraire P.

On va pas cherche une loi exacte mais qui rende les observations le plus vraisemblable possible

Cette quantité L porte le nom de "vraisemblance".
La ou les valeurs du paramètre theta qui maximisent la vraisemblance L(theta) sont appelées "estimateurs
du maximum de vraisemblance" (estimateur MV).

---> ESTIMATEURS DE LA LOI GAMMA

Nous allons utiliser ici une technique appelée "méthode des moments" pour déterminer les estimateurs
des paramètres de la loi Gamma.
Supposons que X1, ..., Xn sont des variables aléatoires indépendantes et identiquement distribuées selon
la loi Gamma avec pour densité:

Le premier moment est l'espérance (a/lambda)
Le second moment est la variance (a(a+1)/lambda^2)

Une fois ce système établie, la méthode des moments consiste à utiliser les moments empiriques, en
l'occurrence pour notre exemple les deux premiers
que l'on pose égaux aux moments théoriques vrais...

=============================================
FACTEUR DE CORRECTION SUR POPULATION FINIE
=============================================

Maintenant démontrons un autre résultat qui nous sera indispensables dans certains tests statistiques
que nous verrons plus loin.

fcp
que nous avons déjà rencontré lors de notre étude la loi hypergéométrique, et appelé "facteur de
correction sur population finie" et il a pour effet de réduire l'erreur-standard d'autant plus que n est
grand.

(multiplie par fcp)

===========================
INTERVALLES DE CONFIANCE
===========================

Jusqu'à maintenant nous avons toujours déterminé les différents estimateurs de vraisemblance ou
estimateurs simples (variance, écart-type) à partir de lois (fonctions) statistiques théoriques ou
mesurées sur toute une population de données.

Définition: Un "intervalle de confiance" est un couple de nombres qui définit une plage de valeurs
possibles avec une certaine probabilité pour un estimateur statistique donné (plage calculée à l'aide de
paramètres vrais mesurés). Il s'agit du cas le plus fréquent en statistiques.

Autrement dit, nous pouvons calculer le nombre n d'individus à mesurer pour s'assurer un intervalle de
confiance donné (associé à Z) de la moyenne mesurée en supposant l'écart-type théorique connu (ou
imposé) et en souhaitant un précision de delta en valeur absolue sur la moyenne.

n = (Z_{alpha/2} sigma)^2/delta

Ces intervalles ont évidemment pour origine que nous travaillons très souvent en statistiques sur des
échantillons et non sur toute la population disponible. L'échantillonage choisi influe donc sur
l'estimateur ponctuel. Nous parlons alors de "fluctuation d'échantillonage".

----

"independant one-sample t-test" (en anglais) ou "test-T à 1 échantillon":
"test-T bilatéral sur la différence de deux moyennes".

"test-T de deux moyennes d'échantillons appariés (ou échantillons dépendants)".

Définition: Nous parlons "d'échantillons appariés" (par paires) si les échantillons de valeurs sont prises
2 fois sur les mêmes individus (donc les valeurs des paires ne sont pas indépendantes, contrairement à
deux échantillons pris indépendamment).

Avant d'aller plus en détails, rappelons qu'il faut être extrêmement prudent quant à la manière d'obtenir
les deux échantillons. Il faut que l'expérience soit non biaisée, cela signifie pour rappel, que le protocole
de tirage ne doit en aucun cas avantager l'une au l'autre des caractéristiques de la population (si vous
étudiez l'équilibre homme/femme dans une population en attirant dans le sondage des personnes grâce à
un cadeau sous la forme de bijoux vous aurez alors un échantillon biaisé... car vous aurez probablement
naturellement plus de femmes que d'hommes...).

Ceci étant fait, pour construire le test et de par l'asymétrie de la distribution, nous allons calculer la
probabilité cumulée que k soit plus petit que le x obtenu par l'expérience et la sommer à la probabilité
cumulée pour que k soit plus grand que le y obtenu par l'expérience (ce qui correspond à la probabilité
cumulée des queues respectivement gauche et droite de la distribution).
et cette dernière relation est appelée "test binomial exact (bilatéral)".

Si la probabilité P obtenue pour la somme est au-dessus d'une certaine probabilité cumulée fixée à
l'avance, nous dirons alors que la différence avec un échantillon tiré au hasard dans une population
parfaitement équilibrée n'est pas significative (en bilatéral...) et respectivement si elle est en-dessous, la
différence sera donc significative et nous rejetterons l'équilibre supposé.

---- TEST DE L'ÉGALITÉ DE DEUX PROPORTIONS

Toujours dans le même contexte que l'approximation précédente de la loi Binomiale par une loi
Normale, l'industrie (en particulier la biostatistique) est friande de comparer deux proportions de deux
populations différentes afin de savoir si elles sont statistiquement égales ou non (autrement dit:
significativement différentes ou pas).

test Z de l'égalité de deux proportions

48/50 et 26/30 ~~~ 95% ?

---- TEST DES SIGNES

Nous mesurons quelque chose sur un échantillon puis, plus tard, nous mesurons la même chose sur ce
même échantillon mais avec une autre méthode (donc il s'agit donc d'échantillons appariés!). Les deux
classements ordonnées des mesures sont comparés et chaque observation est affectée d'un signe ("+"
en cas d'élévation dans le classement, "–" en cas de descente). Celles qui restent au même niveau sont
éliminées.
Selon l'hypothèse à tester, il y a autant de "+" que de "–", c'est-à-dire que la médiane de la distribution
n'a pas bougé (cette affirmation peut ne pas paraître évidente à la première lecture il faut donc bien
prendure du temps parfois pour réfléchir là-dessus).

D1. "L'intervalle de tolérance" (ou "intervalle de fluctuation") est un intervalle contenant un certain
pourcentage (souvent 68.26, 95.44 ou 99.73%) des individus d'une population de mesures.
D2. "L'intervalle de confiance" pour un échantillon de moyenne contient l'intervalle de valeur à un
niveau de confiance donné (souvent 90, 95 ou 99%) de l'espérance (moyenne vraie) de la
population.
D3. "L'intervalle de prédiction" permet de déterminer un intervalle d'un valeur individuelle basée sur la
connaissance de la moyenne échantillonnale et de l'écart-type de la population.

------------

"inégalité de Markov"
"inégalité de Bienaymé-Tchebychev" (abrégée "inégalité BT")

--------- FONCTION CARACTÉRISTIQUE

Avant de donner une démonstration à la manière de l'ingénieur du théorème central limite, introduisons
d'abord le concept de "fonction caractéristique" qui tient une place centrale en statistiques.

rond_barré_x (k) = E[e^{ikX}].


rond_barré_x (k) = \sqrt{2pi} F(-k)
raison pour laquelle la fonction caractéristique de la loi Normale centrée réduite est souvent assimilée à
une simple transformée de Fourier.
F(k) = 1/\sqrt{2pi} e^{-k^2/2}

rond_barré_x (k) = e^{-k^2/2}

--- THÉORÈME CENTRAL LIMITE

Le théorème central limite est un ensemble de résultats du début du 20ème siècle sur la convergence
faible d'une suite de variables aléatoires en probabilité. Intuitivement, d'après ces résultats, toute
somme (implicitement: la moyenne de ses variables) de variables aléatoires indépendantes et
identiquement distribuées tend vers une certaine variable aléatoire.

Le résultat le plus connu et le plus
important est simplement appelé "théorème central limite" qui concerne une somme de variables
aléatoires indépendantes avec variance existante dont le nombre tend vers l'infini
et c'est celui-ci que nous allons démontrer de manière heuristique ici.

Ce théorème de probabilités possède donc une interprétation en statistique mathématique. Cette
dernière associe une loi de probabilité à une population. Chaque élément extrait de la population est
donc considéré comme une variable aléatoire et, en réunissant un nombre n de ces variables supposées
indépendantes, nous obtenons un échantillon. La somme de ces variables aléatoires divisée par n donne
une nouvelle variable nommée la moyenne empirique. Celle-ci, une fois réduite, tend vers une variable
Normale réduite lorsque n tend vers l'infini comme nous le savons.

En deux mots, le Théorème Central Limite (TCL) dit que pour de grands échantillons, la somme
centrée et réduite de n variables aléatoires identiquement distribuées suit une loi Normale centrée et
réduite.

========= TESTS D'HYPOTHÈSE ET D'ADÉQUATION

Définition: Lorsque nous cherchons à savoir si nous pouvons faire confiance à la valeur d'une
statistique (moyenne, médiane, variance, coefficient de corrélation, etc.) avec une certaine certitude,
nous parlons de "test d'hypothèse" et plus particulièrement de "test de conformité" (nous parlons de
"test d'adéquation" quand il s'agit de vérifier que des mesures suivent bien une loi donnée et non juste
une statistique).

