On répète les résultats plusieurs fois sur un échantillon
similaire et on obtient un résultat distribué autour de la moyenne
donc le résultat le plus probable. En pratique, une seule mesure est faite
et il faut déterminer l'erreur faite en choisissant cette valeur comme moyenne.

empiriques = grande population ?

préciser l'intervalle de confiance ainsi que la taille de l'échantillon étudié
et ses caractéristiques détaillées

il est souvent utile et nécessaire de
regrouper les mesures/données dans des intervalles de classe de largeur donnée

Pour éviter d'obtenir une moyenne arithmétique ayant peu de sens, nous calculons
souvent une "moyenne élaguée", c'est à dire une moyenne arithmétique calculée après avoir enlevé
des valeurs aberrantes à la série.
l'utilisation des boîtes à
moustaches (traduction de Box & Whiskers Plot ou BoxPlot) permettant de comparer ("discriminer"
comme disent les spécialistes) rapidement deux populations de données ou plus et surtout d'éliminer les
valeurs aberrantes (prendre comme référence la médiane sera justement plus judicieux!):

====> Fonction Répartition
Soit X une variable aléatoire (v.a.) numérique (quantitative). Elle est complètement décrite par la
valeur de la probabilité (pour les variables discrètes) ou par la probabilité cumulée (pour les variables
continues)
Cette probabilité (cumulée) F(x) s'appelle la "fonction de répartition" de la variable X.
P(a <= x <= b) = F(b) - F(a)

============> Définition: Nous définissons "l'espérance mathématique", appelée aussi "moment d'ordre 1",
appelée aussi "règle des parties".
En d'autres termes, nous savons qu'à chaque événement de l'espace des échantillons est associé une
probabilité à laquelle nous associons également une valeur (donnée par la variable aléatoire). La
question étant alors de savoir quelle valeur, à long terme, nous pouvons obtenir. La valeur espérée,
(l'espérance mathématique donc...) est alors la moyenne pondérée, par la probabilité, de toutes les
valeurs des événements de l'espace des échantillons.

E(aX) = aE(X)
E(X+Y) = E(X)+E(Y)
E(c) = c

Définition: Après avoir traduit la tendance par l'espérance il est intéressant de traduire la dispersion ou
"déviation standard" autour de l'espérance par une valeur appelée "variance de X" ou encore "moment
centré du deuxième ordre", notée V(X) ou sigma^2_X (lire "sigma-deux")

La variance n'est cependant pas comparable directement à la moyenne, car l'unité de la variance est le
carré de l'unité de la variable, ce qui découle directement de sa définition. Pour que l'indicateur de
dispersion puisse être comparé aux paramètres de tendance centrale (moyenne, médiane et... mode), il
suffit d'en prendre la racine carrée.
L'écart-type est donc la moyenne quadratique des écarts (ou "écart moyen quadratique") entre les
observations et leur moyenne.

coefficient de variation : sigma/mu

Le terme de la somme se trouvant dans l'expression de la variance (écart-type) est appelée "somme des
carrés des écarts à la moyenne". Nous l'appelons aussi la "somme des carrés totale", ou encore la
"variation totale" dans le cadre de l'étude de l'ANOVA (voir la fin de ce chapitre).

variable centrée réduite
Y = (X - mu)/sigma

Ainsi, toute répartition statistique définie par une moyenne et un écart-type peut être transformée en
une autre distribution statistique souvent plus simple à analyser.

V(nX) = n^2V(x)
V(X+Y) = V(X) + V(Y) + 2cov(X,Y)

========> ESPÉRANCE ET VARIANCE DE LA MOYENNE (ERREUR STANDARD ET FCP)
Souvent en statistique, il est utile de déterminer l'écart-type de la moyenne empirique (ou en d'autres
termes...: l'erreur quadratique moyenne). Voyons de quoi il s'agit:

en gros si sont identiquement distribuées et indépendantes
alors E(X) = 1/n * n mu
sinon  1/n * somme des espérances

variance : 1/n^2
V(x) = sigma^2/n
--> d'où l'écart-type de la moyenne appelé aussi "erreur-type", "erreur-standard" ou encore "variation non systématique":
sigma(x) = sigma/sqrt(n)

la loi de probabilité de la
variable aléatoire X, moyenne de n variables aléatoires identiquement distribuées et linéairement
indépendantes, est alors la loi:
N(mu, sigma/sqrt(n))

L'analyse du coefficient de corrélation poursuit donc l'objectif de déterminer le degré d'association
entre les différentes variables: celui-ci est souvent exprimé par le coefficient de détermination,
qui est le carré du coefficient de corrélation. Le coefficient de détermination mesure donc
la contribution d'une des variables à l'explication de la seconde.

D1. Nous disons que X est une variable continue si sa "fonction de répartition" est continue (déjà
définie plus haut).

Il est intéressant de remarquer que la définition amène à ce que la probabilité qu'une
variable aléatoire totalement continue prenne une valeur donnée est nulle! Donc ce n'est pas parce
qu'un événement a une probabilité nulle qu'il ne peut arriver!!!

-----------

5. FONCTIONS DE DISTRIBUTIONS = loi
Lorsque nous observons des phénomènes probabilistes, et que nous prenons note des valeurs prises par
ces derniers et que nous les reportons graphiquement, nous observons toujours que les différentes
mesures obtenues suivent une caractéristique courbe ou droite typique fréquemment reproductible.
Dans le domaine des probabilités et statistiques, nous appelons ces caractéristiques des "fonctions de
distribution" car elles indiquent la fréquence avec laquelle la variable aléatoire apparaît avec certaines
valeurs.

Ces fonctions sont en pratique bornées par ce que nous appelons "l'étendue de la distribution", ou
"dispersion de la distribution", qui correspond à la différence entre la donnée maximale (à droite) et la
donnée minimale (à gauche) des valeurs observées (max-min).

----------- 342

D1. La relation mathématique qui donne la probabilité qu'a une variable aléatoire d'avoir une valeur
donnée de la fonction de distribution est appelée "fonction de densité", "fonction de masse" ou encore
"fonction marginale".

D2. La relation mathématique qui donne la probabilité cumulée qu'a une variable aléatoire d'être
inférieure ou égale à une certaine valeur est nommée la "fonction de répartition" ou "fonction
cumulée".

D3. Des variables aléatoires sont dites "indépendantes et identiquement distribuées" (i.i.d.) si elles
suivent toutes la même fonction de distribution et qu'elles sont indépendantes...

---> P(A) = 1/N : "fonction discrète uniforme" (ou "loi discrète uniforme")
E(X) = N+1/2
E(X) = N^2-1/12

----> FONCTION DE BERNOULLI
Ainsi, une variable aléatoire X suit une "fonction de Bernoulli" (ou "loi de Bernoulli") si elle ne peut
prendre que les valeurs 0 ou 1, associées aux probabilités p et q

Remarquons que par extension, si nous considérons N événements où nous obtenons dans un ordre
particulier k fois une des issues possible (réussite) et N-k l'autre (échec), alors la probabilité d'obtenir
une telle série (de k réussites et N-k échecs ordonnés dans un ordre particulier) sera donnée par:
P(x=k) : p^k q^{N-k}
E(x) = p
V(x) = p * q

----> FONCTION GÉOMÉTRIQUE
La loi géométrique ou "loi de Pascal" consiste dans une épreuve de type Bernoulli, dont la probabilité
de succès est p et celle d'échec q=1-p sont constantes, que nous renouvelons de manière
indépendante jusqu'au premier succès.
P(x=k) q^{k-1}*p

E(x)= 1/p
V(x)= (1-p)/p^2

----> FONCTION BINOMIALE

Si nous revenons maintenant à notre épreuve de Bernoulli. Plus généralement, tout N-uplet particulier
formé de k succès et de N-k échecs aura pour probabilité (dans le cadre d'un tirage avec remise ou sans
remise si la population est grande en première approximation...):
P(x=k) = p^k q^{N-k} * (C^k_k)
d'être tiré (ou d'apparaître) quel que soit l'ordre d'apparition des échecs et réussites.

E(x) = np
V(x) = p(1-p)/n

----> FONCTION BINOMIALE NÉGATIVE

La loi binomiale négative s'applique dans la même situation que la loi binomiale mais elle donne la
probabilité d'avoir E échecs avant la R-ème réussite quand la probabilité de succès est p (ou
inversement la probabilité d'avoir R réussites avant le E-ème échec quand la probabilité d'échec est p).

P(ok) = p P(ko)= 1-p = q
p^{nok} * q^{nko} * C^N_k

ce qui correspond donc à la probabilité d'avoir nko échecs avant la nok-ème réussite.

E(x) = 1/p
V(x) = q/p^2

----> FONCTION HYPERGÉOMÉTRIQUE

V(x) = kpq * fpc^2
V(x) = sqrt(kpq) * fpc

fpc = "facteur de correction de population" ou
      en anglais "finite population correction factor"
      = (n-k)/(n-1)

----> FONCTION DE POISSON

Pour certains événements forts rares, la probabilité p est très faible et tend vers zéro. Toutefois la valeur
moyenne n*p tend vers une valeur fixe lorsque n tend vers l'infini.

Nous obtenons ainsi la "fonction de Poisson" (ou "loi de Poisson"), appelée également parfois "loi des
événements rares", donnée donc par:
E(x) = np

Le fait important que pour la loi de Poisson nous ayons la variance qui soit égale à l'espérance est
appelé "propriété d'équidispersion de la de Poisson". Il s'agit d'une propriété souvent utilisée dans la
pratique comme indicateur pour identifier si des données (à support discret) sont distribuées selon une
loi de Poisson.

----> FONCTION DE GAUSS-LAPLACE/LOI NORMALE

Cette caractéristique est la plus importante fonction de distribution en statistiques suite au résultat d'un
théorème connu appelé "théorème central limite" qui comme nous le verrons, permet de démontrer
(entre autres) que toute suite de variables aléatoires indépendantes de même loi ayant une espérance et
un écart-type fini et non nécessairement égales converge vers une fonction de Gauss-Laplace (loi
Normale).

Pour information, une variable suivant une loi Normale centrée réduite est très souvent par tradition
notée Z (pour "Zentriert" en allemand).
En revenant aux variables non normées:

nous obtenons donc la "fonction Gauss-Laplace" (ou "loi de Gauss-Laplace") ou également appelée
"loi Normale" donnée sous forme de densité de probabilité par:
k-u/sigma

Interval 1 sigma (-sigma, sigma) : ~68.3%
Interval 2 sigma (-2sigma, 2sigma) : ~95.4%
Interval 3 sigma (-3sigma, 3sigma) : ~99.7%

La largeur de l'intervalle a une très grande importance dans l'interprétation des incertitudes d'une
mesure. La présentation d'un résultat comme N+n*sigma=p% signifie que la valeur moyenne a environ p% de
chance (probabilité) de se trouver entre les limites N-n*sigma,N+n*sigma

La loi de Gauss-Laplace n'est par ailleurs pas qu'un outil d'analyse de données mais également de
génération de données.

Le fait que la somme de deux lois Normales donne toujours une loi Normale est ce que nous nommons
en statistiques la "stabilité par la somme" de la loi de Gauss-Laplace.

-----> LOI NORMALE CENTRÉE RÉDUITE

k* = k-mu/sigma
LNCR(mu=0,sigma=1)

henry coeff de la droite
0.01k-2
sachant égal à k-mu/sigma
donc mu=200
sigma=100

-----> DIAGRAMME QUANTILE-QUANTILE

Une autre manière de juger qualitativement de l'ajustement de données expérimentales avec une loi
théorique (quelle qu'elle soit!!!) est l'utilisation d'un "diagramme quantile-quantile".

L'idée est assez simple, il s'agit de comparer les données expérimentales, aux données théoriques
supposées suivre une loi donnée.

Et bien évidemment on peut comparer les quantiles observés à toute loi théorique supposée. Plus les
points seront alignés sur la droite, meilleur sera l'ajustement! C'est très visuel, très simple et beaucoup
utilisé par les non spécialistes en statistiques dans les entreprises.

---> FONCTION LOG-NORMALE
y = ln(x)

nous voyons que y suit une fonction de probabilité de type loi Normale de moyenne mu et de variance
sigma^2 (moments de la loi Normale).

---> FONCTION UNIFORME CONTINUE

Soient a < b . Nous définissons la fonction de distribution de la "fonction uniforme" (ou "loi uniforme")
par la relation

P(x) = 1/(b-a) indicatrice[a,b]

E(x) : (a+b)/2
V(x) : (b-a)^2/12

Cette fonction est souvent utilisée en simulation dans les entreprises pour signaler que la
variable aléatoire a des probabilités égales d'avoir une valeur comprise dans un certain intervalle.

---> FONCTION DE PARETO

La "fonction de Pareto" (ou "loi de Pareto"), appelée aussi "loi de puissance" ou encore "loi scalante"
est la formalisation du principe des 80-20. Cet outil d'aide à la décision détermine les facteurs (environ
20%) cruciaux qui influencent la plus grande partie (80%) de l'objectif.

1 - (x_m/x)^k avec x >= x_m

---> FONCTION EXPONENTIELLE

Nous définissons la "fonction exponentielle" (ou "loi exponentielle") par la relation de fonction de
distribution suivante:
lambda * e^{-lambdax} * indicatrice[0,+inf]

E(x) = 1/lambda
V(x) = 1/lambda^2

---> FONCTION DE CAUCHY

Soient X,Y deux variables aléatoires indépendantes suivant des lois Normales centrées réduites
(variance unité et espérance nulle).

Pas d'espérance.

---> LOI BÊTA
---> LOI FONCTION GAMMA

Beta(a)/lambda^a

---> FONCTION DE KHI-DEUX (OU DE PEARSON)

La "fonction de Khi-deux" (appelée aussi "loi du Khi-deux" ou encore "loi de Pearson") n'est par
définition qu'un cas particulier de la fonction de distribution Gamma dans le cas a = k/2 où et
lambda = 1/2, avec k entier positif

E(k) = 2k

pour indiquer que la distribution de la variable aléatoire X est la loi du Khi-deux. Par ailleurs il est
courant de nommer le paramètre k "degré de liberté" et de l'abréger "ddl".

---> FONCTION DE STUDENT T

La "fonction de Student" (ou "loi de Student") de paramètre k est définie par la relation
avec k étant le degré de liberté de la loi du Khi-deux sous-jacente à la construction de la fonction de
Student comme nous allons le voir.

---> FONCTION DE FISHER

. Les paramètres k et l sont des entiers positifs et correspondent aux degrés de liberté des deux
lois du Khi-deux sous-jacentes. Cette distribution est souvent notée F(k,l) et peut être obtenue

==========================================
ESTIMATEURS DE VRAISEMBLANCE
==========================================

Nous supposons que nous disposons d'observations xi qui sont des réalisations de variables
aléatoires non biaisées (dans le sens qu'elles sont choisies aléatoirement parmi un lot) indépendantes
Xi de loi de probabilité inconnue mais identique.

Nous allons chercher à estimer cette loi de probabilité P inconnue à partir des observations

Supposons que nous procédons par tâtonnements pour estimer la loi de probabilité P inconnue. Une
manière de procéder est de se demander si les observations avaient une probabilité
élevée ou non de sortir avec cette loi de probabilité arbitraire P.

On va pas cherche une loi exacte mais qui rende les observations le plus vraisemblable possible

Cette quantité L porte le nom de "vraisemblance".
La ou les valeurs du paramètre theta qui maximisent la vraisemblance L(theta) sont appelées "estimateurs
du maximum de vraisemblance" (estimateur MV).

---> ESTIMATEURS DE LA LOI GAMMA

Nous allons utiliser ici une technique appelée "méthode des moments" pour déterminer les estimateurs
des paramètres de la loi Gamma.
Supposons que X1, ..., Xn sont des variables aléatoires indépendantes et identiquement distribuées selon
la loi Gamma avec pour densité:

Le premier moment est l'espérance (a/lambda)
Le second moment est la variance (a(a+1)/lambda^2)

Une fois ce système établie, la méthode des moments consiste à utiliser les moments empiriques, en
l'occurrence pour notre exemple les deux premiers
que l'on pose égaux aux moments théoriques vrais...

=============================================
FACTEUR DE CORRECTION SUR POPULATION FINIE
=============================================

Maintenant démontrons un autre résultat qui nous sera indispensables dans certains tests statistiques
que nous verrons plus loin.

fcp
que nous avons déjà rencontré lors de notre étude la loi hypergéométrique, et appelé "facteur de
correction sur population finie" et il a pour effet de réduire l'erreur-standard d'autant plus que n est
grand.

(multiplie par fcp)

===========================
INTERVALLES DE CONFIANCE
===========================

Jusqu'à maintenant nous avons toujours déterminé les différents estimateurs de vraisemblance ou
estimateurs simples (variance, écart-type) à partir de lois (fonctions) statistiques théoriques ou
mesurées sur toute une population de données.

Définition: Un "intervalle de confiance" est un couple de nombres qui définit une plage de valeurs
possibles avec une certaine probabilité pour un estimateur statistique donné (plage calculée à l'aide de
paramètres vrais mesurés). Il s'agit du cas le plus fréquent en statistiques.

Autrement dit, nous pouvons calculer le nombre n d'individus à mesurer pour s'assurer un intervalle de
confiance donné (associé à Z) de la moyenne mesurée en supposant l'écart-type théorique connu (ou
imposé) et en souhaitant un précision de delta en valeur absolue sur la moyenne.

n = (Z_{alpha/2} sigma)^2/delta

Ces intervalles ont évidemment pour origine que nous travaillons très souvent en statistiques sur des
échantillons et non sur toute la population disponible. L'échantillonage choisi influe donc sur
l'estimateur ponctuel. Nous parlons alors de "fluctuation d'échantillonage".

----

"independant one-sample t-test" (en anglais) ou "test-T à 1 échantillon":
"test-T bilatéral sur la différence de deux moyennes".

"test-T de deux moyennes d'échantillons appariés (ou échantillons dépendants)".

Avant d'aller plus en détails, rappelons qu'il faut être extrêmement prudent quant à la manière d'obtenir
les deux échantillons. Il faut que l'expérience soit non biaisée, cela signifie pour rappel, que le protocole
de tirage ne doit en aucun cas avantager l'une au l'autre des caractéristiques de la population (si vous
étudiez l'équilibre homme/femme dans une population en attirant dans le sondage des personnes grâce à
un cadeau sous la forme de bijoux vous aurez alors un échantillon biaisé... car vous aurez probablement
naturellement plus de femmes que d'hommes...).

Ceci étant fait, pour construire le test et de par l'asymétrie de la distribution, nous allons calculer la
probabilité cumulée que k soit plus petit que le x obtenu par l'expérience et la sommer à la probabilité
cumulée pour que k soit plus grand que le y obtenu par l'expérience (ce qui correspond à la probabilité
cumulée des queues respectivement gauche et droite de la distribution).
et cette dernière relation est appelée "test binomial exact (bilatéral)".

Si la probabilité P obtenue pour la somme est au-dessus d'une certaine probabilité cumulée fixée à
l'avance, nous dirons alors que la différence avec un échantillon tiré au hasard dans une population
parfaitement équilibrée n'est pas significative (en bilatéral...) et respectivement si elle est en-dessous, la
différence sera donc significative et nous rejetterons l'équilibre supposé.

---- TEST DE L'ÉGALITÉ DE DEUX PROPORTIONS

Toujours dans le même contexte que l'approximation précédente de la loi Binomiale par une loi
Normale, l'industrie (en particulier la biostatistique) est friande de comparer deux proportions de deux
populations différentes afin de savoir si elles sont statistiquement égales ou non (autrement dit:
significativement différentes ou pas).

test Z de l'égalité de deux proportions

48/50 et 26/30 ~~~ 95% ?

---- TEST DES SIGNES

Nous mesurons quelque chose sur un échantillon puis, plus tard, nous mesurons la même chose sur ce
même échantillon mais avec une autre méthode (donc il s'agit donc d'échantillons appariés!). Les deux
classements ordonnées des mesures sont comparés et chaque observation est affectée d'un signe ("+"
en cas d'élévation dans le classement, "–" en cas de descente). Celles qui restent au même niveau sont
éliminées.
Selon l'hypothèse à tester, il y a autant de "+" que de "–", c'est-à-dire que la médiane de la distribution
n'a pas bougé (cette affirmation peut ne pas paraître évidente à la première lecture il faut donc bien
prendure du temps parfois pour réfléchir là-dessus).

D1. "L'intervalle de tolérance" (ou "intervalle de fluctuation") est un intervalle contenant un certain
pourcentage (souvent 68.26, 95.44 ou 99.73%) des individus d'une population de mesures.
D2. "L'intervalle de confiance" pour un échantillon de moyenne contient l'intervalle de valeur à un
niveau de confiance donné (souvent 90, 95 ou 99%) de l'espérance (moyenne vraie) de la
population.
D3. "L'intervalle de prédiction" permet de déterminer un intervalle d'un valeur individuelle basée sur la
connaissance de la moyenne échantillonnale et de l'écart-type de la population.

------------

"inégalité de Markov"
"inégalité de Bienaymé-Tchebychev" (abrégée "inégalité BT")

--------- FONCTION CARACTÉRISTIQUE

Avant de donner une démonstration à la manière de l'ingénieur du théorème central limite, introduisons
d'abord le concept de "fonction caractéristique" qui tient une place centrale en statistiques.

rond_barré_x (k) = E[e^{ikX}].


rond_barré_x (k) = \sqrt{2pi} F(-k)
raison pour laquelle la fonction caractéristique de la loi Normale centrée réduite est souvent assimilée à
une simple transformée de Fourier.
F(k) = 1/\sqrt{2pi} e^{-k^2/2}

rond_barré_x (k) = e^{-k^2/2}

--- THÉORÈME CENTRAL LIMITE

Le théorème central limite est un ensemble de résultats du début du 20ème siècle sur la convergence
faible d'une suite de variables aléatoires en probabilité. Intuitivement, d'après ces résultats, toute
somme (implicitement: la moyenne de ses variables) de variables aléatoires indépendantes et
identiquement distribuées tend vers une certaine variable aléatoire.

Le résultat le plus connu et le plus
important est simplement appelé "théorème central limite" qui concerne une somme de variables
aléatoires indépendantes avec variance existante dont le nombre tend vers l'infini
et c'est celui-ci que nous allons démontrer de manière heuristique ici.

Ce théorème de probabilités possède donc une interprétation en statistique mathématique. Cette
dernière associe une loi de probabilité à une population. Chaque élément extrait de la population est
donc considéré comme une variable aléatoire et, en réunissant un nombre n de ces variables supposées
indépendantes, nous obtenons un échantillon. La somme de ces variables aléatoires divisée par n donne
une nouvelle variable nommée la moyenne empirique. Celle-ci, une fois réduite, tend vers une variable
Normale réduite lorsque n tend vers l'infini comme nous le savons.

En deux mots, le Théorème Central Limite (TCL) dit que pour de grands échantillons, la somme
centrée et réduite de n variables aléatoires identiquement distribuées suit une loi Normale centrée et
réduite.

========= TESTS D'HYPOTHÈSE ET D'ADÉQUATION

Définition: Lorsque nous cherchons à savoir si nous pouvons faire confiance à la valeur d'une
statistique (moyenne, médiane, variance, coefficient de corrélation, etc.) avec une certaine certitude,
nous parlons de "test d'hypothèse" et plus particulièrement de "test de conformité" (nous parlons de
"test d'adéquation" quand il s'agit de vérifier que des mesures suivent bien une loi donnée et non juste
une statistique).

Les tests d'hypothèses sont destinés à vérifier si un échantillon peut être considéré comme extrait d'une
population donnée ou représentatif de cette population, vis-à-vis d'un paramètre comme la moyenne, la
variance ou la fréquence observée. Ceci implique que la loi théorique du paramètre soit connue au
niveau de la population. Les tests d'hypothèses ne sont pas faits pour démontrer l'hypothèse nulle
(exprimant généralement une égalité ou une homogénéité entre différentes populations), mais pour
éventuellement la rejeter (dispons pour être exact que le rejet est plus robuste).

Nous parlons du "test-Z de la moyenne à deux échantillons" et il est beaucoup utilisé dans l'industrie
pour vérifier l'égalité de la moyenne de deux populations de mesures.

Et si l'écart-type théorique n'est pas connu, nous utiliserons le "test-T" de Student (pas mal utilisé en
pharmaco-économie)

Dans la même idée pour l'écart-type, nous utiliserons le "test du Khi-deux"

lorsque nous voulons tester l'égalité de la variance de deux populations nous utilisons le "test-F"
de Fisher

Dans la pratique il faut avoir conscience que le but d'un test est très très souvent de montrer que l'effet
est significatif. Il est alors d'usage de dire que le test réussit si l'hypothèse nulle est rejetée au profit de
l'hypothèse alternative. Lorsque le praticien sait que l'effet est significatif et pourtant que son test

Signalons aussi que les tests d'hypothèses sur l'écart-type (variance), la moyenne ou la corrélation sont
appelés des "tests paramétriques" à l'inverse des tests non paramétriques que nous verrons beaucoup
plus loin.

Enfin, de nombreux logiciels calculent ce que nous appelons la "p-value" qui est le risque calculé
(probabilité) alpha (ou t) qu'aurait pu fixer le statisticien pour être à la limite entre l'acceptation de l'hypothèse
nulle et son rejet (rappelons qu'un test qui réussit ne prouve rien). La p-value est donc une valeur
fondamentale dans le domaine car elle permet de chiffrer la vraisemblance de l'hypothèse nulle H0
(acception ou rejet).
Pour un test d'hypothèse, par exemple, le 5% de risque est celui de rejeter l'hypothèse nulle H0
alors même qu'elle est vraie. Si le risque imposé/choisi est 5% et que la p-value calculée est inférieure
(dans la majorité des tests mais il faut être prudent car ce n'est pas une généralité!!!), le test échoue
(rejet de l'hypothèse) en faveur d'une hypothèse alternative notée ou parfois .

Remarque: Il existe un type d'ANOVA prévu pour le cas où les variables ne sont pas indépendantes
(on parle alors de "covariable"). Il s'agit de l'ANCOVA qui signifie "Analyse de la COvariance et de
la VAriance" qui utilise un mix entre la régression linéaire (cf. chapitre de Méthodes Numériques) et
l'ANOVA. Le but de l'ANCOVA est de supprimer statistiquement l'effet indirect de la covariable.

----- ANALYSE DE LA VARIANCE (À UN FACTEUR)

L'objectif de l'analyse de la variance (contrairement à ce que son nom pourrait laisser penser) est une
technique statistique permettant de comparer les moyennes de deux populations ou plus (très utilisé
dans le pharma ou dans les labos de R&D ou de bancs d'essais). Cette méthode, néanmoins, doit son
nom au fait qu'elle utilise des mesures de variance afin de déterminer le caractère significatif, ou non,
des différences de moyennes mesurées sur les populations.
Plus précisément, la vraie signification est de savoir si le fait que des moyennes d'échantillons sont
(légèrement) différentes peut être attribué au hasard de l'échantillonnage ou provient du fait qu'un
facteur de variabilité engendre réellement des échantillons significativement différents (si nous avons
les valeurs de toute la population, nous n'avons rien à faire!).

Pour l'analyse de la variance appelée "ANOVA à un facteur" (ANalysis Of VAriance) ou "ANAVAR à
un facteur" (ANAlyse de la VARiance), ou encore "ANOVA à une voie" ou plus rigoureusement
"ANOVA à un facteur fixe avec répétitions" ou encore "ANOVA à une variable catégorielle fixe avec
répétition", nous allons d'abord rappeler, comme nous l'avons démontré, que la loi de Fisher-Snedecor
est donnée par le rapport de deux variables aléatoires indépendantes qui suivent une loi du Khi-deux et
divisée par leur degré de liberté tel que:

Pour valider l'utilité d'un facteur, il met au point un test permettant
d'assurer que des échantillons différents sont de natures différentes. Ce test est basé sur l'analyse de la
variance (des échantillons), et nommé ANOVA (analyse normalisée de la variance).
Prenons k échantillons de n valeurs aléatoires chacun. Chacune des valeurs étant considérée comme
une observation ou une mesure de quelque chose ou sur la base de quelque chose (un lieu différent, ou
un objet différent... bref: un seul et unique facteur de variabilité entre les échantillons!). Nous aurons
donc un nombre total de N d'observations (mesures) donné par:

N = kn
si chacun des échantillons a un nombre identique de valeurs n (taille de l'échantillon) tel que
nous parlons alors de "plan équilibré" à k niveaux (ou k modalités)

En termes de test, nous voulons tester si les moyennes des k échantillons de taille n sont égales sous
l'hypothèse que leurs variances sont égales.

Autrement dit: les échantillons sont représentatifs d'une même population (d'une même loi statistique).
C'est-à-dire que les variations constatées entre les valeurs des différents échantillons sont dues
essentiellement au hasard. Pour cela nous étudions la variabilité des résultats dans les échantillons et
entre les échantillons.

La "variance totale" comme étant intuitivement la variance estimée sans biais en considérant
l'ensemble des N observations comme un seul échantillon:
où le terme au numérateur est appelé "somme des carrés des écarts totaux".

2. La "variance entre échantillons" (c'est-à-dire entre les moyennes des échantillons) est aussi
intuitivement l'estimateur de la variance des moyennes des échantillons:
où le terme au numérateur est appelé "somme des carrés des écarts entre échantillons".

3. La "variance résiduelle" est l'effet des facteurs dits non contrôlés. C'est par définition la moyenne des
variances des échantillons (en quelque sorte: l'erreur standard):
où le terme au numérateur est appelé "somme des carrés des écarts des résidus" ou encore plus souvent
"erreur résiduelle".

R1. Le terme est souvent indiqué dans l'industrie par l'abréviation SST signifiant en anglais
"Sum of Squares Total" ou plus rarement TSS pour "Total Sum of Squares".
R2. Le terme est souvent indiqué dans l'industrie par l'abréviation SSB signifiant en anglais
"Sum of Squares Between (samples)" ou plus rarement SSk pour "Sum of Squares Between
treatments".
R3. Le terme est souvent indiqué dans l'industrie par l'abréviation SSW signifiant en anglais
"Sum of Squares Within (samples)" ou plus rarement SSE pour "Sum of Squares due to Errors".

Cette dernière relation est appelée "independent 2-sample T-test", ou "test-T
homoscédastique" ou encore "test-T d'égalité des espérances de 2 observations avec variances
égales" ou encore plus simplement mais un peu abusivement "test-T à 2 échantillons", avec taille
des échantillons différentes et variances égales. Souvent dans la littérature, les deux moyennes
théoriques sont égales lors de la comparaison.

Il faut cependant bien se rappeler que pour utiliser l'ANOVA, on doit donc supposer que les
échantillons sont issus d'une même population (données appariées) et suivent une loi normale. Il est
donc nécessaire de vérifier la normalité des distributions et l'homoscédasticité (test de Levene par
exemple). Dans le cas contraire, il faut utiliser des variantes non paramétriques de l'analyse de variance
(ANOVA de Kruskal-Wallis ou ANOVA de Friedman).

------------ test khi deux

Pour évaluer ces écarts et pouvoir prendre une décision, il faut:
1. Définir la mesure de la distance entre distribution empirique et distribution théorique résultant de la
loi retenue.
2. Déterminer la loi de probabilité suivie par cette variable aléatoire donnant la distance.
3. Énoncer une règle de décision permettant de dire, d'après la distribution observée, si la loi retenue est
acceptable ou non.

De ce que j'ai compris
- on a des fréquences=observations~critère pour des critères
- on a une somme des observations
- en théorie, tous les fréquences sont égales donc on a
- fréquence_th = observations/#critères
Il s'agit donc d'examiner
la différence entre les fréquences observées et les fréquences théoriques (supposées suivre une loi
uniforme) en suivant la relation du Khi-deux. En d'autres termes, nous allons faire un test d'ajustement
entre une fonction de distribution empirique (observée) et la fonction de distribution uniforme. Nous
avons alors
(observation_i - observation_th_i)^2
/
observation_th_i
pour tout critère
la somme est ... = chi^2
% qui indique le % que ce soit probable.

============= Robustesse

D1. Un test est dit "test robuste" s'il reste valable alors que les hypothèses d'application ne sont pas
toutes réunies. Ce peut être une taille d'échantillon un peu faible ou une loi de probabilité (loi normale
pour les tests paramétriques) qui n'est pas très bien vérifiée. Par exemple, l'ANOVA est robuste par
rapport à l'hypothèse de normalité mais pas par rapport à celle de l'homoscédasticité

Par conséquent, à moins d'être uniquement descriptives, vos études devront respecter quelques règles
pour que leurs conclusions soient généralisables.
Première condition d'une bonne robustesse: les données. Intuitivement, chacun sait qu'on ne transforme
pas un cas en généralité (ce qui ne relèverait pas des statistiques mais des discussions de comptoir). Une
quantité suffisante de données permet de bâtir des modèles fiables et solides. À titre d'exemple, des
prévisions établies à partir d'une série chronologique montrant une saisonnalité nécessitent au moins
trois ou quatre ans d'historique.
La quantité ne suffit pas, il faut la qualité. Mieux vaut s'abstenir que réaliser une étude sur des
informations non fiables qui peuvent conduire à des décisions coûteuses. Par ailleurs, il convient
d'éliminer ou d'imputer certaines observations (voir outliers). Si ce n'est pas possible, on se tourne vers
des méthodes adaptées, par exemple celles qui utilisent la médiane plutôt que la moyenne.

================== STATISTIQUES DE RANGS

Les statistiques de rangs, appelées aussi "statistiques d'ordre", sont définies comme l'ensemble des
techniques de calculs statistiques ou d'inférence statistiques qui ont pour objectif principal de se
débarrasser de la connaissance d'une distribution paramétrée et en utilisant pour cela uniquement les
rangs (ordonnés) des caractéristiques mesurées. Il s'agit d'un outil très puissant et très utilisé dans la
pratique pour faire de la statistique non-paramétrée!

---------------

(1) Test-T de Student
Type:
Test d'hypothèse paramétrique de type intervalle de confiance souvent utilisé en bilatéral
Concerne:
La moyenne lorsque l'écart-type théorique est inconnu
Contrainte(s):
Distribution Normale des données.
(2) Test-p de l'intervalle de confiance de proportions
Type:
Test d'hypothèse paramétrique de type intervalle de confiance souvent utilisé en bilatéral
Concerne:
La proportion de bons ou mauvais éléments dans une population
Contrainte(s):
Distribution Binomiale (et asymptotiquement) Normale des données (n*p>=5).

(3) Test-p de l'égalité de deux proportions
Type:
Test d'hypothèse paramétrique de type intervalle de confiance souvent utilisé en unilatéral
Concerne:
L'égalité de deux proportions
Contrainte(s):
Distribution Binomiale (et asymptotiquement) Normale des données (n*p>=5).

(4) Test binomial exact (égalité de deux proportions)
Type:
Test d'hypothèse paramétrique de type intervalle de confiance souvent utilisé en bilatéral
Concerne:
L'égalité de deux proportions.
Contrainte(s):
Distribution Binomiale (petit échantillon d'un grande population)
(5) Test des signes (de la médiane) de deux échantillons appariés
Type:
Test d'hypothèse non-paramétrique de type intervalle de confiance souvent utilisé en bilatéral
Concerne:
L'égalité des signes (implicitement des différences) de données appariées
Contrainte(s):
Distribution Binomiale (petit échantillon d'un grande population) mais valeurs sous-jacentes continues.
(6) Test-T de Student de deux moyennes d'échantillons appariés
Type:
Test d'hypothèse paramétrique de type intervalle de confiance souvent utilisé unilatéral.
Concerne:
La différence de deux moyennes de deux échantillons identiques
Contrainte(s):
Distribution Normale des données
(7) Test-Z
Type:
Test d'hypothèse paramétrique de type intervalle de confiance plus utilisé en bilatéral qu'en unilatéral.
Concerne:
La moyenne lorsque l'écart-type théorique est connu
Contrainte(s):
Distribution Normale des données

8) Test-Z de la moyenne à deux échantillons
Type:
Test d'hypothèse paramétrique de type intervalle de confiance souvent utilisé en bilatéral
Concerne:
La différence de deux moyennes lorsque les écarts-types théoriques sont connus
Contrainte(s):
Distribution Normale des données
(9) Test du Khi-deux
Type:
Test d'hypothèse paramétrique de type intervalle de confiance souvent utilisé en bilatéral
Concerne:
La variance théorique
Contrainte(s):
Distribution Normale des données
(10) Test-F de Fisher
Type:
Test d'hypothèse paramétrique de type intervalle de confiance souvent utilisé en bilatéral
Concerne:
La comparaison de deux variances théoriques
Contrainte(s):
Distribution Normale des données
(11) Test-T homoscédastique
Type:
Test d'hypothèse paramétrique de type intervalle de confiance souvent utilisé en unilatéral
Concerne:
L'égalité de deux moyennes
Contrainte(s):
Distribution Normale des données et égalité des variances expérimentales

(12) Test-T hétéroscédastique
Type:
Test d'hypothèse paramétrique de type intervalle de confiance souvent utilisé en unilatéral
Concerne:
L'égalité de deux moyennes
Contrainte(s):
Distribution Normale des données et non-égalité des variances expérimentales (cas généralisé du Test-T
homoscédastique)
(13) Test de l'ANOVA à un facteur contrôlé
Type:
Test d'hypothèse paramétrique de type intervalle de confiance utilisé uniquement en unilatéral
Concerne:
L'égalité des moyennes des échantillons (supposés implicitement appariés)
Contrainte(s):
Distribution Normale des données avec variances théoriques identiques et variances expérimentales connues et
indépendance des échantillons. Les résidus doivent in extenso aussi être normalement distributés
(14) Test de l'ANOVA à deux facteurs contrôles avec ou sans répétition
Type:
Test d'hypothèse paramétrique de type intervalle de confiance utilisé uniquement en unilatéral
Concerne:
L'égalité des moyennes des échantillons fonction d'un paramètre variable contrôlable (ajustable).
Contrainte(s):
Distribution Normale des données avec variances théoriques identiques et variances expérimentales connues et
indépendance des échantillons. Les résidus doivent in extenso aussi être normalement distributés.
(15) Test d'ajustement (dit aussi "test d'adéquation de Pearson") du Khi-deux
Type:
Test d'ajustement paramétrique utilisé uniquement en unilatéral
Concerne:
Adéquation de valeurs expérimentales à une loi théorique
Contrainte(s):
Avoir suffisamment de classes d'intervalles et de données
Remarque: Appelé "Test de normalité" si comparé à une loi Normale.

(16) Test d'indépendance du Khi-deux
Type:
Test d'ajustement (étudié dans le chapitre de Méthodes Numériques) paramétrique utilisé uniquement en
unilatéral
Concerne:
Vérifier la dépendance ou l'indépendance (différence) de données provenant d'une table de contingence. Vérifie
donc si les moyennes sont différentes ou pas entre groupes en se basant sur la contingence
Contrainte(s):
Avoir suffisamment de classes d'intervalles et de données
(17) Test de la médiane
Type:
Test d'hypothèse non paramétrique étudié dans le chapitre de Méthodes Numériques de type intervalle de
confiance toujours utilisé en bilatéral
Concerne:
La médiane
Contrainte(s):
Un nombre d'échantillons suffisant pour faire un bootstrap.

(19) Test de Poisson à un et deux échantillons
Type:
Tests d'hypothèses paramétrique tantôt en unilatéral ou bilatéral basé sur les événements rares (dixit la moyenne
de la loi de Poisson)
Concerne:
Déterminer un intervalle de confiance pour l'occurrence d'événements rares sur une période donnée afin
d'identifier une anomalie ou une différence significative par rapport à des objectifs ou des nromes.
Contrainte(s):
Les événements suivent une loi de Poisson mais sont approximés dans le cas à deux échantillons par une loi
Normale...

(24) Test exact de Fisher
Type:
Test d'ajustement (étudié dans le chapitre de Méthodes Numériques) paramétrique utilisé principalement en
bilatérial
Concerne:
Vérifier si la configuration observée dans un tableau de contingence est une situation extrême par rapport aux
situations possibles.
Contrainte(s):
Aucune en particulière

Les tests non paramétriques (comme les deux tests du Khi-deux déjà vus) ne font eux aucune
hypothèse sur la distribution sous-jacente des données. L'étape préalable qui consistait uniquement à
estimer les paramètres des distributions avant de procéder au test d'hypothèse proprement dit n'est plus
nécessaire.
Lorsque les données sont quantitatives, les tests non paramétriques transforment les valeurs en rangs.
L'appellation "tests de rangs" est alors souvent rencontrée. Lorsque les données sont qualitatives, seuls
les tests non paramétriques sont utilisables.

========== RANG

on ordonne toutes les valeurs de X et Y
la première est R1 (rang 1), si deux valeurs == alors même i-ème de rang

On fait la somme des i pour chaque X/Y : Valeurs que nous appelons "statistique de Wilcoxon".

l'espérance de la loi discrète uniforme
(n+1)/2 = mu
n^2-1/12 = V(x)

---------- TEST DE LA SOMME DES RANGS DE MANN-WITHNEY

Le "test de la somme des rangs de Mann-Withney" est au fait un test d'ajustement non-paramétrique
très simple qui se déduit du test de la somme des rangs de Wilcoxon. Par ailleurs il en est inspiré à un
tel point que nous l'appelons parfois dans l'industrie le "test de Wilcoxon-Mann-Withney" ou "test
d'ajustement de Wilcoxon-Mann-Withney" ou encore"test MWW" (sans spécifier à chaque fois qu'il
repose sur la somme des rangs).

Le but de ce test, identiquement au test de la somme des rangs de Wilcoxon, est de trouver un moyen
de vérifier que deux échantillons indépendants non nécessairement de même taille sont issus d'une
même loi ou non (in extenso sont issus d'une même population ou non) mais avec une approche
différente!

---------- TEST DE LA SOMME DES RANGS SIGNÉS DE WILCOXON À 1 ÉCHANTILLON

Le but du test de la "somme des rangs signés de Wilcoxon", appelé aussi parfois "test de la médiane de
Wilcoxon", est d'utiliser une technique non paramétrique pour vérifier la symétrie ou non d'une
distribution et donc in extenso faire une hypothèse sur la valeur de la médiane. L'idée est à la fois
simple et subtile.

Le principe et que si nous comparons les différences des individus d'un échantillon par rapport à la
médiane, nous savons que si nous avons (par exemple) un nombre impair d'individus tous différents
(non égaux), alors nous aurons 50% des données au-dessus et en-dessous de la médiane. Ensuite, pour
contrôler que la distribution des valeurs des individus vérifie une certaine symétrie, l'idée (simple mais
astucieuse) consiste ensuite à:
1. Calculer les différences en valeur absolue par rapport à la médiane
2. Ranger ces différences absolues par ordre croissant et leur assigner leur rang respectif
3. Calculer la somme des rangs des différences qui à la base sont négatives
4. Calculer la somme des rangs des différences qui à la base sont positives
et si l'échantillon a une distribution symétrique (donc la médiane est confondue alors avec la moyenne),
il devrait y avoir une somme des rangs négatifs qui n'est pas significativement différente de la
sommes des rangs positifs .

------- TEST DE LA SOMME DES RANGS SIGNÉS DE WILCOXON POUR 2 ÉCHANTILLONS APPARIÉS
Le "test de la somme des rangs signés de Wilcoxon pour 2 échantillons appariés" est basé à 100% sur le
principe du test à 1 échantillon. La seule différence est que l'hypothèse nulle ou alternative est basée
sur la différence de la médiane des données prises deux à deux de chacun des échantillons. Dans la
majorité des cas, l'hypothèse nulle est que la médiane des différences est nulle contre l'hypothèse
alternative qu'elle est significativement différente de zéro.

-------- TEST DE KRUSKAL-WALLIS

Le test de Kruskal-Wallis un test non paramétrique souvent assimilé (un peu rapidement...) à une
ANOVA non paramétrique à une voie pour comparer si deux populations ou plus ont même médiane
(hypothèse nulle) à la différence qu'il ne nécessite donc pas les hypothèses nécessaires au
fonctionnement de l'ANOVA. Quand plusieurs populations comparées passent à travers ce test, ce
dernier ne dit pas quelle population est significativement différente mais uniquement qu'il y en a au
moins une qui l'est. En réalité, comme nous allons le démontrer, le test de Kruskal-Wallis n'est qu'une
extension du test U de Mann-Whitney vu plus haut pour un nombre de populations supérieur ou égal à
trois.

============================= CALCULS D'ERREURS/INCERTITUDES

Il est impossible de connaître (mesurer) la valeur exacte d'une grandeur physique expérimentalement, il
est très important donc d'en déterminer l'incertitude.
Nous appelons bien évidemment "erreur", la différence entre la valeur mesurée et la valeur exacte.
Cependant, comme nous ignorons la valeur exacte, nous ne pouvons pas connaître l'erreur commise
quand même.... Le résultat est donc toujours incertain. C'est la raison pour laquelle nous parlons des
"incertitudes de mesure".
Nous distinguons deux types d'incertitudes:
1. Les "erreurs systématiques": elles affectent le résultat constamment et dans le même sens (erreurs
des appareils de mesures, limites de précision, etc.). Il faut alors éliminer, ou corriger le résultat, si
possible !
2. Les "erreurs accidentelles" (statistiques): il faut alors répéter les mesures, calculer la moyenne et
évaluer l'incertitude en utilisant les outils de statistique.
Le deuxième type d'erreurs fait un très gros usage de tous les outils statistiques que nous avons
présentés jusqu'à maintenant. Nous ne reviendrons donc pas dessus et nous nous concentrerons alors
uniquement sur quelques nouveaux concepts.