Statistiques pondérées
	Si on a des valeurs associés à une probabilités dans deux vecteurs, alors on peut soit
	les fusionner soit travailler dessus sans les désagréger avec :code:`wtd.mean(v,p)`,
	:code:`wtd.mean(v,p)`, ... du package :code:`questionr`.

On peut également tester la fonction de répartition (:code:`ecdf(rloi(...))` en R)
en la superposant à un graphique précédemment obtenu (on rappelle le add=TRUE).
----> tracer médiane, ...

structure
-----------------

Sous-partie d'un dataframe, subset(stid,Sexe==2,select=c(Groupe,Sexe,Note.Fr.))
-> select : variables gardées, -> stid : fichier, -> sexe==2 : condition

library(lattice)	regroup<-make.groups(nom=valeur, ...)

order(vecteur) : donne les indexes dans l'ordre

================================
================================
============= Corrélation
================================
================================

Ajustement linéaire
	Coefficient de corrélation (cor) et une droite de régression (lm)
	coefficients(modele)
	Le coefficient de corrélation linéaire est assez élevé (un test le prouverait).
	Le nuage a une forme allongée, on peut donc effectuer un ajustement linéaire.
	Il est possible de récupérer les valeurs des résidus (residuals(modele)) et les valeurs
	y prédites (fitted(modele)).

	Matrices de graphiques (pairs, GGally)
		Étudier la liaison entre plusieurs variables numériques

	Matrice des corrélations
		Repérer les liens entre les variables quantitatives.

Test (de nullité) du coefficient de corrélation linéaire
	| :code:`Prérequis` : test de normalité ou plus de 30 individus, deux variables quantitatives
	| :code:`Résultat` : 0 = corrélation possible, ou valeur entre -1 et 1
	| :code:`Calcul` : :code:`cor.test()` (test de corrélation)

	Matrice des corrélations
		On peut utiliser :code:`cor.mtest(data)$p` du package :code:`corrplot`
		pour voir la matrice des corrélations, avec data une matrice ou un data.frame
		par exemple. On peut voir les corrélations deux à deux.

	On peut utiliser :code:`corrplot(cor(data), method="circle")` ou
	:code:`corrplot(cor(data), method="number")` du package :code:`corrplot`
	pour avoir un aperçu graphique.

8. Méthodes supervisées
=============================

Dans le problème d'apprentissage non supervisé, nous n'observons que les caractéristiques des individus et
n'avons aucune variable cible. L’objectif est de mieux « connaître » les individus et les variables.
Nous avons vu cela dans les méthodes précédentes.

Dans les méthodes supervisées, une variable est privilégiée. Cette variable cible va servir
à guider l’apprentissage à partir des données.

Méthodes supervisées classiques.
Si la variable cible Y est quantitative, la régression linéaire est incontournable.
Les arbres de décision sont utilisés aussi.

Si la variable cible est qualitative, la régression logistique, l’analyse discriminante,
les arbres de décision, et leur généralisation : les forêts aléatoires, sont utilisés.

---> Régression linéaire
Cette méthode statistique permet d’étudier la liaison entre une variable numérique32 Y et un ensemble
de variables X1, X2,...,Xp indépendantes, non corrélées entre elles autant que faire se peut 33. La variable
Y est donc la variable cible. La régression est donc une méthode supervisée. Elle permet de bien comprendre
les facteurs agissant sur une variable particulière, par contre, elle n’est pas très efficace en prévision.

Dans le cas de deux variables : X et Y, on parle de régression linaire simple.
Il s’agit alors de trouver la « meilleure » droite permettant d’ajuster Y en X.

En gros avoir données et tracer droite qui se rapproche de données.
L'origine du terme «régression» pour décrire le processus d'ajustement d’une droite aux données.

Si Y est qualitative ou discrète, il vaut mieux se rabattre sur la régression logistique.
Nous effectuons la régression sur le modèle complet grâce à l’instruction lm() :
fit<-lm(Y~X1+X2+X3+X4,data=froment)
summary(fit) # synthèse des résultats

La p-valeur (0,029) nous dit que la liaison globale entre Y et les Xiest significative. (Nous rejetons
l’hypothèse : [H0 = la liaison n’est pas significative] ).

R donne ici des paramètres importants comme l’estimation de l’écart type commun résiduel  : s=1,24
(racine carrée de MSerreur). C’est ce qui va nous servir à prédire Y. Plus RSE (ou S) est petit, meilleure sera la
précision du modèle.
Nous trouvons aussi le R² qui est le quotient de la variance expliquée par la régression par la variance totale.
Ici 79,5% de la variance totale de la variable Y est expliquée par le modèle.
Multiple R-squared: 0.7948
Le R² ajusté (0.658) est très utile pour comparer des modèles entre eux.

Notre problème vient du fait que les régresseurs sont corrélés entre eux : vérifiez-le en calculant la matrice
des corrélations des régresseurs.

Nous voyons que les variables X2,X3 et X4 sont corrélées entre elles (alpha=5%). Elles ne peuvent donc pas toutes être dans le modèle ! Il faut faire un tri.
Par contre, nous voyons que tous les Xi sont corrélés à Y et c’est une bonne chose !

Grâce à regsubsets de l’extension leaps nous allons rechercher un modèle plus adéquat. On privilégiera
les modèles à faible s ou à fort r² ajusté.
library(leaps)
meilleur<-regsubsets(Y~X1+X2+X3+X4,data=froment,nbest=10)
# visualisation des résultats
summary(meilleur)
X 3 "*"
Ainsi, le meilleur modèle à 1 variable est X3, le meilleur à 2 variables est X1 X4 etc.

cases noires le best
plot(meilleur,scale="adjr2",main="Modèles classés en fonction de r²ajusté")
on regarde pour r^2 ajusté (axe y)
On afficher cela plus clairement avec : library(car) subsets(meilleur, statistic="adjr2")

je crois qu'on cherche des valeurs <5%

Analyse des résidus et des valeurs influentes (extension MASS)
Calcul de la distance de Cook, du DFFITS et du résidu par validation croisée.

library(MASS)
#Nous faisons l’ajustement avec la librarie MASS
fit <- lm(Y~X1+X4,data=froment) data.frame(froment$Y, froment$X1, froment$X4,
cook=cooks.distance(fit),dffits=dffits(fit), std.res=stdres(fit), std.res.valX=studres(fit))

Supposons que cette année X1=109 et X4=900. Je désire prévoir le rendement à l’aide du modèle précédent.
Nous créons d’abord un mini fichier contenant les données en question :
donneesprev<-data.frame(X1=109,X4=900)
predict(fit,donneesprev,interval="predict",level=0.95)
Le rendement prévu est donc compris entre 21.7 et 25.3 (à 95%).

Graphiques avec l’extension olsrr (à passer en 1re lecture)
Distance de Cook
Elle mesure l’influence de l’observation sur les coefficients de régression.
#Distance de Cook library(olsrr) ols_cooksd_chart(fit)
Les observations 2 et 7 sont donc influentes. Il faut s’assurer
que ce ne sont pas des erreurs de saisie sinon l’équation de régression serait faussée.
DFFITS
Il mesure l’influence sur l’équation et l’erreur. #DFFITS library(olsrr) ols_dffits_plot(fit)

Résidus studentisés par validation croisée (Deleted T residual)
J’ai repris la terminologie de mon collègue et ami le Pr. Anestis Antoniadis pour ce résidu particulier.
Il est calculé en retirant l’observation du modèle ce qui permet d’avoir une estimation
plus fiable de l’erreur.

library(olsrr) ols_dsrvsp_plot(fit)
L’observation 7 est aberrante (et influente).
L’observation 2 est juste influente.

Stop page 302/451