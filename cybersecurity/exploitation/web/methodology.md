# Methodology to exploit webservers

<div class="row row-cols-md-2"><div>

**Website mapping** üìå

As a penetration tester, you will have to list every feature of the website, and their route. For instance, "list all products": `/products,` "list a product": `/products?id=x`. This is done as you explore by BurpSuite if you are redirecting requests through the proxy, even if you don't intercept them.

<br>

**Look at the source of the website** (<kbd>CTRL+U</kbd>)

[![picklerick](../../_badges/thm-p/picklerick.svg)](https://tryhackme.com/room/picklerick)

* is there hidden elements?
* is there information in HTML comments?

<br>

**Look for logic flaws**

A logic flaw is when a programmer is thinking that a user will do something, such as browsing pages in a specific order, or submitting an email in an "email input field", and does not check, or not properly check, for those that take a path different from intended.

Another logic flaw could be related to websites using a username to check if a user is allowed to see the "admin" side of the website. Maybe some tricks like an username " admin" may bypass the check.

<br>

**Look for frameworks** [See frameworks](frameworks/index.md)

Most developers are using a framework, which is a sort of box of utilities to make a website more easily.

<br>

**Analyze the network**

You may use the dev console network tab to analyze requests, and responses sent to the server. Once you click on a request, there are multiple tabs, for requests, responses...

Some misconfigured servers are, for instance, sending the web service name <small>(Nginx, Apache, IIS, Node.js)</small>, and sometimes even the version.

<br>

**Analyze the javascript**

You may use the dev console debugger, after adding a breakpoint in the JavaScript, to analyze the javascript code, if needed.

<br>

**/robots.txt and /sitemap.xml**

[![picklerick](../../_badges/thm-p/picklerick.svg)](https://tryhackme.com/room/picklerick)

Robots.txt is listing pages that crawlers (search engine robots) shouldn't crawl. There may be some routes that even clients were not aware of inside.

There is also a link to the website "sitemap", which is generated by the developer to help search engine index the website. If the sitemap wasn't updated, you may find interesting routes, meaning not available for the "usual" visitor, but that were available before, that were not removed, and that are maybe vulnerable.
</div><div>

**Forced Browsing** [See Forced Browsing](fuzz/forced_browsing.md)

This is a technique of not using the website links to navigate to another page, but rather trying manually/using a tool with a list of routes that may exist, and trying them all while hoping to find some gold.

<br>

**Insecure Direct Object References (IDOR)** [See fuzzing](fuzz/index.md)

[![idor](../../_badges/thm/idor.svg)](https://tryhackme.com/room/idor)
[![neighbour](../../_badges/thm-p/neighbour.svg)](https://tryhackme.com/room/neighbour)

A failure in which we can guess given a URL how to access other elements,
and if we change the URL <small>(id=10$\to$id=11)</small>. It could be a get form with predictable values, or a folder with predictable filenames... It could also be unpredictable values, and in such case, the best test would be to create two accounts, and see if you can swap the potential IDOR. They can be found in forms, in the JavaScript, in cookies...

<br>

**Find usernames/emails** [See Brute force forms](forms/index.md)

You could achieve this using the website API if there is one, and/or its interface, such as register forms: "this username is already taken", or the same process for emails with the password forgot page...

<br>

**Wayback machine**

You can look for previous versions of the website stored on the [Wayback machine](https://archive.org/web/), if any. Google has also a feature to see cached versions of a website, but it's not very useful for this purpose.

<br>

**OSINT**

If the company tried to hire employees in the past, you may find information about the framework or stuff like this in the job offers.

If you can find employees on GitHub/..., you may find what language/frameworks they are using for their websites.

You may also look for personal websites/posts/interests of employees, because that may be an indicator too.
</div></div>

<hr class="sep-both">

## üé£ Toolsüé£

<div class="row row-cols-md-2"><div>

* [whatweb](https://github.com/urbanadventurer/WhatWeb) (4.2k ‚≠ê). Web scanner.

```bash
$ whatweb URL
# Server / Libraries used
# Headers
```
</div><div>

* [nikto](https://github.com/sullo/nikto) (6.2k ‚≠ê). Scan web server for known vulnerabilities.

```bash
$ nikto -h URL
# Server
# SSL certificate
# Headers
```
</div></div>

<div class="row row-cols-md-2 mt-3"><div>

* Spoof User-Agent

You can install an extension to make the target website believe that you are using another browser. Look for **User-Agent Switcher** extensions on Google. You can try them on [whatismybrowser](https://www.whatismybrowser.com/).

* Use a proxy

You may use a proxy as an intermediary for your requests. If you do, then you can use the **FoxyProxy extension** of your browser to easily swap between no proxy, and your proxies configurations.
</div><div>

* Disable scripts

You may do that to bypass JavaScript verifications. You can use plugins such as [noscript](https://noscript.net/), or ublock by clicking on the following icon

![ublock_disable_scripts](_images/ublock_disable_scripts.png)

* [wappalyzer](https://github.com/wappalyzer/wappalyzer) (7.8k ‚≠ê) to find frameworks, their versions... It can also be installed as a browser extension or used via their [website](https://www.wappalyzer.com/).

* [Beef](https://github.com/beefproject/beef) (7.8k ‚≠ê): The Browser Exploitation Framework Project
</div></div>

<hr class="sep-both">

## Mitigations

<div class="row row-cols-md-2 mt-4"><div>

* üîí Display generic error messages, disable errors messages, and do not give much information <small>(ex: on invalid login, display the message 'credentials invalid', instead of 'username invalid' or 'password invalid')</small>

* üöß Test your endpoints with invalid values: 0, -1, characters, symbols...
</div><div>

* üî´ Do not trust anything coming from the user, its browser, or even your database. Basically, Zero Trust.
</div></div>